{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "<table align=\"left\">\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/vertex-ai-samples/blob/main/notebooks/community/feature_store/mobile_gaming/mobile_gaming_feature_store.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/feature_store/mobile_gaming/mobile_gaming_feature_store.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/feature_store/mobile_gaming/mobile_gaming_feature_store.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>                                                                                               \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    " \n",
    "Imagine you are a member of the Data Science team working on the same Mobile Gaming application reported in the [Churn prediction for game developers using Google Analytics 4 (GA4) and BigQuery ML](https://cloud.google.com/blog/topics/developers-practitioners/churn-prediction-game-developers-using-google-analytics-4-ga4-and-bigquery-ml) blog post.\n",
    " \n",
    "Business wants to use that information in real-time to take immediate intervention actions in-game to prevent churn. In particular, for each player, they want to provide gaming incentives like new items or bonus packs depending on the customer demographic,  behavioral information and the resulting propensity of return.\n",
    " \n",
    "Last year, Google Cloud announced Vertex AI, a managed machine learning (ML) platform that allows data science teams to accelerate the deployment and maintenance of ML models. One of the platform building blocks is Vertex AI Feature store which provides a managed service for low latency scalable feature serving. Also it is a centralized feature repository with easy APIs to search & discover features and feature monitoring capabilities to track drift and other quality issues.\n",
    " \n",
    "In this notebook, we will show how the role of Vertex AI Feature Store in a ready to production scenario when the user's activities within the first 24 hours of last engagement and the gaming platform would consume in order to improve UX. Below you can find the high level picture of the system\n",
    " \n",
    "<img src=\"./mobile_gaming_architecture_1.png\">\n",
    " \n",
    " \n",
    "### Dataset\n",
    " \n",
    "The dataset is the public sample export data from an actual mobile game app called \"Flood It!\" (Android, iOS)\n",
    " \n",
    "### Objective\n",
    " \n",
    "In the following notebook, you will learn how Vertex AI Feature store\n",
    " \n",
    "1.  Provide a centralized feature repository with easy APIs to search & discover features and fetch them for training/serving.\n",
    " \n",
    "2.  Simplify deployments of models for Online Prediction, via low latency scalable feature serving.\n",
    " \n",
    "3.  Mitigate training serving skew and data leakage by performing point in time lookups to fetch historical data for training.\n",
    " \n",
    "**Notice that we assume that already know how to set up a Vertex AI Feature store. In case you are not, please check out [this detailed notebook](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/feature_store/gapic-feature-store.ipynb).**\n",
    " \n",
    " \n",
    "### Costs\n",
    " \n",
    "This tutorial uses billable components of Google Cloud:\n",
    " \n",
    "* Vertex AI\n",
    "* BigQuery\n",
    "* Cloud Storage\n",
    " \n",
    "Learn about [Vertex AI\n",
    "pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage\n",
    "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ze4-nDLfK4pw"
   },
   "source": [
    "### Set up your local development environment\n",
    "\n",
    "**If you are using Colab or Vertex AI Workbench Notebooks**, your environment already meets\n",
    "all the requirements to run this notebook. You can skip this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCuSR8GkAgzl"
   },
   "source": [
    "**Otherwise**, make sure your environment meets this notebook's requirements.\n",
    "You need the following:\n",
    "\n",
    "* The Google Cloud SDK\n",
    "* Git\n",
    "* Python 3\n",
    "* virtualenv\n",
    "* Jupyter notebook running in a virtual environment with Python 3\n",
    "\n",
    "The Google Cloud guide to [Setting up a Python development\n",
    "environment](https://cloud.google.com/python/setup) and the [Jupyter\n",
    "installation guide](https://jupyter.org/install) provide detailed instructions\n",
    "for meeting these requirements. The following steps provide a condensed set of\n",
    "instructions:\n",
    "\n",
    "1. [Install and initialize the Cloud SDK.](https://cloud.google.com/sdk/docs/)\n",
    "\n",
    "1. [Install Python 3.](https://cloud.google.com/python/setup#installing_python)\n",
    "\n",
    "1. [Install\n",
    "   virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv)\n",
    "   and create a virtual environment that uses Python 3. Activate the virtual environment.\n",
    "\n",
    "1. To install Jupyter, run `pip3 install jupyter` on the\n",
    "command-line in a terminal shell.\n",
    "\n",
    "1. To launch Jupyter, run `jupyter notebook` on the command-line in a terminal shell.\n",
    "\n",
    "1. Open this notebook in the Jupyter Notebook Dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "### Install additional packages\n",
    "\n",
    "Install additional package dependencies not installed in your notebook environment, such as XGBoost. Use the latest major GA version of each package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2b4ef9b72d43"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Vertex AI Workbench Notebook product has specific requirements\n",
    "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\")\n",
    "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
    "    \"/opt/deeplearning/metadata/env_version\"\n",
    ")\n",
    "\n",
    "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_WORKBENCH_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "_vr6BYED_5my"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m  WARNING: The scripts pip, pip3, pip3.10 and pip3.7 are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script tb-gcp-uploader is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting git+https://github.com/googleapis/python-aiplatform.git@main\n",
      "  Cloning https://github.com/googleapis/python-aiplatform.git (to revision main) to /tmp/pip-req-build-7_2zyo5b\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/googleapis/python-aiplatform.git /tmp/pip-req-build-7_2zyo5b\n",
      "  Resolved https://github.com/googleapis/python-aiplatform.git to commit 414e39b46acb9c49cb7b650b4e168c78bc6c49d2\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in /home/jupyter/.local/lib/python3.7/site-packages (from google-cloud-aiplatform==1.18.3) (2.10.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /home/jupyter/.local/lib/python3.7/site-packages (from google-cloud-aiplatform==1.18.3) (1.22.1)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /home/jupyter/.local/lib/python3.7/site-packages (from google-cloud-aiplatform==1.18.3) (3.20.3)\n",
      "Requirement already satisfied: packaging<22.0.0dev,>=14.3 in /home/jupyter/.local/lib/python3.7/site-packages (from google-cloud-aiplatform==1.18.3) (21.3)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /home/jupyter/.local/lib/python3.7/site-packages (from google-cloud-aiplatform==1.18.3) (1.44.0)\n",
      "Requirement already satisfied: google-cloud-bigquery<3.0.0dev,>=1.15.0 in /home/jupyter/.local/lib/python3.7/site-packages (from google-cloud-aiplatform==1.18.3) (2.34.4)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /home/jupyter/.local/lib/python3.7/site-packages (from google-cloud-aiplatform==1.18.3) (1.6.3)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /home/jupyter/.local/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.18.3) (1.35.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /home/jupyter/.local/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.18.3) (2.28.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /home/jupyter/.local/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.18.3) (1.56.4)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /home/jupyter/.local/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.18.3) (1.48.2)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /home/jupyter/.local/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.18.3) (1.50.0)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /home/jupyter/.local/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.18.3) (2.4.0)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /home/jupyter/.local/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.18.3) (2.8.2)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /home/jupyter/.local/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.18.3) (2.3.2)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /home/jupyter/.local/lib/python3.7/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform==1.18.3) (0.12.4)\n",
      "Requirement already satisfied: six in /home/jupyter/.local/lib/python3.7/site-packages (from google-cloud-storage<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.18.3) (1.16.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/jupyter/.local/lib/python3.7/site-packages (from packaging<22.0.0dev,>=14.3->google-cloud-aiplatform==1.18.3) (3.0.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/jupyter/.local/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.18.3) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/jupyter/.local/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.18.3) (4.9)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.18.3) (59.8.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/jupyter/.local/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.18.3) (4.2.4)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /home/jupyter/.local/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform==1.18.3) (1.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jupyter/.local/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.18.3) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/jupyter/.local/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.18.3) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/jupyter/.local/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.18.3) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jupyter/.local/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.18.3) (2022.9.24)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/jupyter/.local/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform==1.18.3) (0.4.8)\n",
      "Building wheels for collected packages: google-cloud-aiplatform\n",
      "  Building wheel for google-cloud-aiplatform (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for google-cloud-aiplatform: filename=google_cloud_aiplatform-1.18.3-py2.py3-none-any.whl size=2271535 sha256=45873f12bf4e3f31f8a15a04bc952e2fdf3185a9e544fe0dba1fd5d48e31f854\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-g8r4u1b2/wheels/d2/0e/d8/992c12a48396016b58fdd417c0414021de6574b236ecff6db8\n",
      "Successfully built google-cloud-aiplatform\n",
      "Installing collected packages: google-cloud-aiplatform\n",
      "  Attempting uninstall: google-cloud-aiplatform\n",
      "    Found existing installation: google-cloud-aiplatform 1.11.0\n",
      "    Uninstalling google-cloud-aiplatform-1.11.0:\n",
      "      Successfully uninstalled google-cloud-aiplatform-1.11.0\n",
      "\u001b[33m  WARNING: The script tb-gcp-uploader is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed google-cloud-aiplatform-1.18.3\n"
     ]
    }
   ],
   "source": [
    "! pip3 install {USER_FLAG} --upgrade pip -q\n",
    "! pip3 install {USER_FLAG} --upgrade google-cloud-aiplatform==1.11.0 -q --no-warn-conflicts\n",
    "! pip3 install {USER_FLAG} git+https://github.com/googleapis/python-aiplatform.git@main # For features monitoring\n",
    "! pip3 install {USER_FLAG} --upgrade google-cloud-bigquery==2.24.0 -q --no-warn-conflicts\n",
    "! pip3 install {USER_FLAG} --upgrade xgboost==1.1.1 -q --no-warn-conflicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhq5zEbGg0XX"
   },
   "source": [
    "### Restart the kernel\n",
    "\n",
    "After you install the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "EzrelQZ22IZj"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWEdiXsJg0XY"
   },
   "source": [
    "## Before you begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF1j6f9HApxa"
   },
   "source": [
    "### Set up your Google Cloud project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "1. [Enable the APIs](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,notebooks.googleapis.com, ). \n",
    "\n",
    "1. If you are running this notebook locally, you will need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
    "\n",
    "1. Enter your project ID in the cell below. Then run the cell to make sure the\n",
    "Cloud SDK uses the right project for all the commands in this notebook.\n",
    "\n",
    "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oM1iC_MfAts1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID:  uki-mlops-dev-demo\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ID = \"uki-mlops-dev-demo\"\n",
    "\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJYoRfYng0XZ"
   },
   "source": [
    "Otherwise, set your project ID here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "riG_qUokg0XZ"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
    "    PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "set_gcloud_project_id"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23988890fef6"
   },
   "source": [
    "#### Get your project number (Optional)\n",
    "\n",
    "Now that the project ID is set, you get your corresponding project number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2d6950574e1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Number: 106131389347\n"
     ]
    }
   ],
   "source": [
    "shell_output = ! gcloud projects list --filter=\"PROJECT_ID:'{PROJECT_ID}'\" --format='value(PROJECT_NUMBER)'\n",
    "PROJECT_NUMBER = shell_output[0]\n",
    "print(\"Project Number:\", PROJECT_NUMBER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "#### Region\n",
    "\n",
    "You can also change the `REGION` variable, which is used for operations\n",
    "throughout the rest of this notebook.  Below are regions supported for Vertex AI. We recommend that you choose the region closest to you.\n",
    "\n",
    "- Americas: `us-central1`\n",
    "- Europe: `europe-west4`\n",
    "- Asia Pacific: `asia-east1`\n",
    "\n",
    "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
    "\n",
    "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "jIcZV7-C2RrX"
   },
   "outputs": [],
   "source": [
    "REGION = \"us-central1\"  # @param {type: \"string\"}\n",
    "\n",
    "if REGION == \"[your-region]\":\n",
    "    REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06571eb4063b"
   },
   "source": [
    "#### Timestamp\n",
    "\n",
    "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append it onto the name of resources you create in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "697568e92bd6"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dr--iN2kAylZ"
   },
   "source": [
    "### Authenticate your Google Cloud account\n",
    "\n",
    "**If you are using Vertex AI Workbench Notebooks**, your environment is already\n",
    "authenticated. Skip this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBCra4QMA2wR"
   },
   "source": [
    "**If you are using Colab**, run the cell below and follow the instructions\n",
    "when prompted to authenticate your account via oAuth.\n",
    "\n",
    "**Otherwise**, follow these steps:\n",
    "\n",
    "1. In the Cloud Console, go to the [**Create service account key**\n",
    "   page](https://console.cloud.google.com/apis/credentials/serviceaccountkey).\n",
    "\n",
    "2. Click **Create service account**.\n",
    "\n",
    "3. In the **Service account name** field, enter a name, and\n",
    "   click **Create**.\n",
    "\n",
    "4. In the **Grant this service account access to project** section, click the **Role** drop-down list and add the following roles:\n",
    "   - BigQuery Admin\n",
    "   - Storage Admin\n",
    "   - Storage Object Admin\n",
    "   - Vertex AI Administrator\n",
    "   - Vertex AI Feature Store Admin\n",
    "\n",
    "\n",
    "5. Click *Create*. A JSON file that contains your key downloads to your\n",
    "local environment.\n",
    "\n",
    "6. Enter the path to your service account key as the\n",
    "`GOOGLE_APPLICATION_CREDENTIALS` variable in the cell below and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyQmSRbKA8r-"
   },
   "outputs": [],
   "source": [
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# If on Vertex AI Workbench, then don't execute this code\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\") and not os.getenv(\n",
    "    \"DL_ANACONDA_HOME\"\n",
    "):\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "Set the name of your Cloud Storage bucket below. It must be unique across all\n",
    "Cloud Storage buckets.\n",
    "\n",
    "You may also change the `REGION` variable, which is used for operations\n",
    "throughout the rest of this notebook. Make sure to [choose a region where Vertex AI services are\n",
    "available](https://cloud.google.com/vertex-ai/docs/general/locations#available_regions). You may\n",
    "not use a Multi-Regional Storage bucket for training with Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"vertex-experiments-dl\"  # @param {type:\"string\"}\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cf221059d072"
   },
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"[your-bucket-name]\":\n",
    "    BUCKET_NAME = PROJECT_ID + \"-aip-\" + TIMESTAMP\n",
    "    BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EcIXiGsCePi"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "NIq7R4HZCfIc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://vertex-experiments-dl/...\n",
      "ServiceException: 409 A Cloud Storage bucket named 'vertex-experiments-dl' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n"
     ]
    }
   ],
   "source": [
    "! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucvCsknMCims"
   },
   "source": [
    "Finally, validate access to your Cloud Storage bucket by examining its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "vhOb7YnwClBb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 gs://vertex-experiments-dl/data/\n",
      "                                 gs://vertex-experiments-dl/deliverables/\n",
      "                                 gs://vertex-experiments-dl/iris/\n",
      "                                 gs://vertex-experiments-dl/model/\n",
      "                                 gs://vertex-experiments-dl/pipelines/\n"
     ]
    }
   ],
   "source": [
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "set_service_account"
   },
   "source": [
    "#### Service Account (Optional)\n",
    "\n",
    "If you do not want to use your project's Compute Engine service account, set `SERVICE_ACCOUNT` to another service account ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MQVV9haf2Rra"
   },
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autoset_service_account"
   },
   "outputs": [],
   "source": [
    "if (\n",
    "    SERVICE_ACCOUNT == \"\"\n",
    "    or SERVICE_ACCOUNT is None\n",
    "    or SERVICE_ACCOUNT == \"[your-service-account]\"\n",
    "):\n",
    "    # Get your service account from gcloud\n",
    "    if not IS_COLAB:\n",
    "        shell_output = !gcloud auth list 2>/dev/null\n",
    "        SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
    "\n",
    "    else:  # IS_COLAB:\n",
    "        shell_output = ! gcloud projects describe  $PROJECT_ID\n",
    "        project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "        SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "\n",
    "    print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "set_service_account:pipelines"
   },
   "source": [
    "#### Set service account access\n",
    "\n",
    "Run the following commands to grant your service account access. You only need to run this step once per service account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4UpQThc2Rrb"
   },
   "outputs": [],
   "source": [
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
    "\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "utO91mebwEmw"
   },
   "source": [
    "### Create a Bigquery dataset\n",
    "\n",
    "You create the BigQuery dataset to store the data along the demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "8615339fa4ca"
   },
   "outputs": [],
   "source": [
    "BQ_DATASET = \"Mobile_Gaming\"  # @param {type:\"string\"}\n",
    "LOCATION = \"US\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "G3G2BXqswb_J"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigQuery error in mk operation: Dataset 'uki-mlops-dev-demo:Mobile_Gaming'\n",
      "already exists.\n"
     ]
    }
   ],
   "source": [
    "!bq mk --location=$LOCATION --dataset $PROJECT_ID:$BQ_DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "pRUOFELefqf1"
   },
   "outputs": [],
   "source": [
    "# General\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Data Science\n",
    "import pandas as pd\n",
    "# Vertex AI and its Feature Store\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.aiplatform import Feature, Featurestore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgP_N3MSpnd3"
   },
   "source": [
    "### Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "b_k2ejxQsET8"
   },
   "outputs": [],
   "source": [
    "# Data Engineering and Feature Engineering\n",
    "TODAY = \"2022-06-16\"\n",
    "LABEL_TABLE = f\"label_table_{TODAY}\".replace(\"-\", \"\")\n",
    "FEATURES_TABLE = f\"wide_features_table_{TODAY}\"  # @param {type:\"string\"}\n",
    "FEATURESTORE_ID = \"mobile_gaming\"  # @param {type:\"string\"}\n",
    "ENTITY_TYPE_ID = \"user\"\n",
    "\n",
    "# Vertex AI Feature store\n",
    "ONLINE_STORE_NODES_COUNT = 5\n",
    "ENTITY_ID = \"user\"\n",
    "API_ENDPOINT = f\"{REGION}-aiplatform.googleapis.com\"\n",
    "FEATURE_TIME = \"timestamp\"\n",
    "ENTITY_ID_FIELD = \"user_pseudo_id\"\n",
    "BQ_SOURCE_URI = f\"bq://{PROJECT_ID}.{BQ_DATASET}.{FEATURES_TABLE}\"\n",
    "GCS_DESTINATION_PATH = f\"data/features/train_features_{TODAY}\".replace(\"-\", \"\")\n",
    "GCS_DESTINATION_OUTPUT_URI = f\"{BUCKET_URI}/{GCS_DESTINATION_PATH}\"\n",
    "SERVING_FEATURE_IDS = {\"user\": [\"*\"]}\n",
    "READ_INSTANCES_TABLE = f\"ground_truth_{TODAY}\".replace(\"-\", \"\")\n",
    "READ_INSTANCES_URI = f\"bq://{PROJECT_ID}.{BQ_DATASET}.{READ_INSTANCES_TABLE}\"\n",
    "\n",
    "# Vertex AI Training\n",
    "BASE_CPU_IMAGE = \"us-docker.pkg.dev/vertex-ai/training/scikit-learn-cpu.0-23:latest\"\n",
    "DATASET_NAME = f\"churn_mobile_gaming_{TODAY}\".replace(\"-\", \"\")\n",
    "TRAIN_JOB_NAME = f\"xgb_classifier_training_{TODAY}\".replace(\"-\", \"\")\n",
    "MODEL_NAME = f\"churn_xgb_classifier_{TODAY}\".replace(\"-\", \"\")\n",
    "MODEL_PACKAGE_PATH = \"train_package\"\n",
    "TRAINING_MACHINE_TYPE = \"n1-standard-4\"\n",
    "TRAINING_REPLICA_COUNT = 1\n",
    "DATA_PATH = f\"{GCS_DESTINATION_OUTPUT_URI}/000000000000.csv\".replace(\"gs://\", \"/gcs/\")\n",
    "MODEL_PATH = f\"model/{TODAY}\".replace(\"-\", \"\")\n",
    "MODEL_DIR = f\"{BUCKET_URI}/{MODEL_PATH}\".replace(\"gs://\", \"/gcs/\")\n",
    "\n",
    "# Vertex AI Prediction\n",
    "DESTINATION_URI = f\"{BUCKET_URI}/{MODEL_PATH}\"\n",
    "VERSION = \"v1\"\n",
    "SERVING_CONTAINER_IMAGE_URI = (\n",
    "    \"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.0-23:latest\"\n",
    ")\n",
    "ENDPOINT_NAME = \"mobile_gaming_churn\"\n",
    "DEPLOYED_MODEL_NAME = f\"churn_xgb_classifier_{VERSION}\"\n",
    "MODEL_DEPLOYED_NAME = \"churn_xgb_classifier_v1\"\n",
    "SERVING_MACHINE_TYPE = \"n1-highcpu-4\"\n",
    "MIN_NODES = 1\n",
    "MAX_NODES = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "hY99VsxQBOU9"
   },
   "outputs": [],
   "source": [
    "# Sampling distributions for categorical features implemented in\n",
    "# https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/model_monitoring/model_monitoring.ipynb\n",
    "\n",
    "LANGUAGE = [\n",
    "    \"en-us\",\n",
    "    \"en-gb\",\n",
    "    \"ja-jp\",\n",
    "    \"en-au\",\n",
    "    \"en-ca\",\n",
    "    \"de-de\",\n",
    "    \"en-in\",\n",
    "    \"en\",\n",
    "    \"fr-fr\",\n",
    "    \"pt-br\",\n",
    "    \"es-us\",\n",
    "    \"zh-tw\",\n",
    "    \"zh-hans-cn\",\n",
    "    \"es-mx\",\n",
    "    \"nl-nl\",\n",
    "    \"fr-ca\",\n",
    "    \"en-za\",\n",
    "    \"vi-vn\",\n",
    "    \"en-nz\",\n",
    "    \"es-es\",\n",
    "]\n",
    "\n",
    "OS = [\"IOS\", \"ANDROID\", \"null\"]\n",
    "COUNTRY = [\n",
    "    \"United States\",\n",
    "    \"India\",\n",
    "    \"Japan\",\n",
    "    \"Canada\",\n",
    "    \"Australia\",\n",
    "    \"United Kingdom\",\n",
    "    \"Germany\",\n",
    "    \"Mexico\",\n",
    "    \"France\",\n",
    "    \"Brazil\",\n",
    "    \"Taiwan\",\n",
    "    \"China\",\n",
    "    \"Saudi Arabia\",\n",
    "    \"Pakistan\",\n",
    "    \"Egypt\",\n",
    "    \"Netherlands\",\n",
    "    \"Vietnam\",\n",
    "    \"Philippines\",\n",
    "    \"South Africa\",\n",
    "]\n",
    "\n",
    "USER_IDS = [\n",
    "    \"C8685B0DFA2C4B4E6E6EA72894C30F6F\",\n",
    "    \"A976A39B8E08829A5BC5CD3827C942A2\",\n",
    "    \"DD2269BCB7F8532CD51CB6854667AF51\",\n",
    "    \"A8F327F313C9448DFD5DE108DAE66100\",\n",
    "    \"8BE7BF90C971453A34C1FF6FF2A0ACAE\",\n",
    "    \"8375B114AFAD8A31DE54283525108F75\",\n",
    "    \"4AD259771898207D5869B39490B9DD8C\",\n",
    "    \"51E859FD9D682533C094B37DC85EAF87\",\n",
    "    \"8C33815E0A269B776AAB4B60A4F7BC63\",\n",
    "    \"D7EA8E3645EFFBD6443946179ED704A6\",\n",
    "    \"58F3D672BBC613680624015D5BC3ADDB\",\n",
    "    \"FF955E4CA27C75CE0BEE9FC89AD275A3\",\n",
    "    \"22DC6A6AE86C0AA33EBB8C3164A26925\",\n",
    "    \"BC10D76D02351BD4C6F6F5437EE5D274\",\n",
    "    \"19DEEA6B15B314DB0ED2A4936959D8F9\",\n",
    "    \"C2D17D9066EE1EB9FAE1C8A521BFD4E5\",\n",
    "    \"EFBDEC168A2BF8C727B060B2E231724E\",\n",
    "    \"E43D3AB2F9B9055C29373523FAF9DB9B\",\n",
    "    \"BBDCBE2491658165B7F20540DE652E3A\",\n",
    "    \"6895EEFC23B59DB13A9B9A7EED6A766F\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTjqqE0jafnn"
   },
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "9dJ_TGTVak7u"
   },
   "outputs": [],
   "source": [
    "def run_bq_query(query: str):\n",
    "    \"\"\"\n",
    "    An helper function to run a BigQuery job\n",
    "    Args:\n",
    "        query: a formatted SQL query\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        job = bq_client.query(query)\n",
    "        _ = job.result()\n",
    "    except RuntimeError as error:\n",
    "        print(error)\n",
    "\n",
    "\n",
    "def upload_model(\n",
    "    display_name: str,\n",
    "    serving_container_image_uri: str,\n",
    "    artifact_uri: str,\n",
    "    sync: bool = True,\n",
    ") -> vertex_ai.Model:\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        display_name: The name of Vertex AI Model artefact\n",
    "        serving_container_image_uri: The uri of the serving image\n",
    "        artifact_uri: The uri of artefact to import\n",
    "        sync:\n",
    "\n",
    "    Returns: Vertex AI Model\n",
    "\n",
    "    \"\"\"\n",
    "    model = vertex_ai.Model.upload(\n",
    "        display_name=display_name,\n",
    "        artifact_uri=artifact_uri,\n",
    "        serving_container_image_uri=serving_container_image_uri,\n",
    "        sync=sync,\n",
    "    )\n",
    "    model.wait()\n",
    "    print(model.display_name)\n",
    "    print(model.resource_name)\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_endpoint(display_name: str) -> vertex_ai.Endpoint:\n",
    "    \"\"\"\n",
    "    An utility to create a Vertex AI Endpoint\n",
    "    Args:\n",
    "        display_name: The name of Endpoint\n",
    "\n",
    "    Returns: Vertex AI Endpoint\n",
    "\n",
    "    \"\"\"\n",
    "    endpoint = vertex_ai.Endpoint.create(display_name=display_name)\n",
    "\n",
    "    print(endpoint.display_name)\n",
    "    print(endpoint.resource_name)\n",
    "    return endpoint\n",
    "\n",
    "\n",
    "def deploy_model(\n",
    "    model: vertex_ai.Model,\n",
    "    machine_type: str,\n",
    "    endpoint: vertex_ai.Endpoint = None,\n",
    "    deployed_model_display_name: str = None,\n",
    "    min_replica_count: int = 1,\n",
    "    max_replica_count: int = 1,\n",
    "    sync: bool = True,\n",
    ") -> vertex_ai.Model:\n",
    "    \"\"\"\n",
    "    An helper function to deploy a Vertex AI Endpoint\n",
    "    Args:\n",
    "        model: A Vertex AI Model\n",
    "        machine_type: The type of machine to serve the model\n",
    "        endpoint: An Vertex AI Endpoint\n",
    "        deployed_model_display_name: The name of the model\n",
    "        min_replica_count: Minimum number of serving replicas\n",
    "        max_replica_count: Max number of serving replicas\n",
    "        sync: Whether to execute method synchronously\n",
    "\n",
    "    Returns: vertex_ai.Model\n",
    "\n",
    "    \"\"\"\n",
    "    model_deployed = model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        deployed_model_display_name=deployed_model_display_name,\n",
    "        machine_type=machine_type,\n",
    "        min_replica_count=min_replica_count,\n",
    "        max_replica_count=max_replica_count,\n",
    "        sync=sync,\n",
    "    )\n",
    "\n",
    "    model_deployed.wait()\n",
    "\n",
    "    print(model_deployed.display_name)\n",
    "    print(model_deployed.resource_name)\n",
    "    return model_deployed\n",
    "\n",
    "\n",
    "def endpoint_predict_sample(\n",
    "    instances: list, endpoint: vertex_ai.Endpoint\n",
    ") -> vertex_ai.models.Prediction:\n",
    "    \"\"\"\n",
    "    An helper function to get prediction from Vertex AI Endpoint\n",
    "    Args:\n",
    "        instances: The list of instances to score\n",
    "        endpoint: An Vertex AI Endpoint\n",
    "\n",
    "    Returns:\n",
    "        vertex_ai.models.Prediction\n",
    "\n",
    "    \"\"\"\n",
    "    prediction = endpoint.predict(instances=instances)\n",
    "    print(prediction)\n",
    "    return prediction\n",
    "\n",
    "\n",
    "def generate_online_sample() -> dict:\n",
    "    \"\"\"\n",
    "    An helper function to generate a sample of online features\n",
    "    Returns:\n",
    "        online_sample: dict of online features\n",
    "    \"\"\"\n",
    "    online_sample = {}\n",
    "    online_sample[\"entity_id\"] = random.choices(USER_IDS)\n",
    "    online_sample[\"country\"] = random.choices(COUNTRY)\n",
    "    online_sample[\"operating_system\"] = random.choices(OS)\n",
    "    online_sample[\"language\"] = random.choices(LANGUAGE)\n",
    "    return online_sample\n",
    "\n",
    "\n",
    "def simulate_prediction(endpoint: vertex_ai.Endpoint, n_requests: int, latency: int):\n",
    "    \"\"\"\n",
    "    An helper function to simulate online prediction with customer entity type\n",
    "        - format entities for prediction\n",
    "        - retrieve static features with a singleton lookup operations from Vertex AI Feature store\n",
    "        - run the prediction request and get back the result\n",
    "    Args:\n",
    "        endpoint: Vertex AI Endpoint object\n",
    "        n_requests: number of requests to run\n",
    "        latency: latency in seconds\n",
    "    Returns:\n",
    "        vertex_ai.models.Prediction\n",
    "    \"\"\"\n",
    "    for i in range(n_requests):\n",
    "        online_sample = generate_online_sample()\n",
    "        online_features = pd.DataFrame.from_dict(online_sample)\n",
    "        entity_ids = online_features[\"entity_id\"].tolist()\n",
    "\n",
    "        customer_aggregated_features = user_entity_type.read(\n",
    "            entity_ids=entity_ids,\n",
    "            feature_ids=[\n",
    "                \"cnt_user_engagement\",\n",
    "                \"cnt_level_start_quickplay\",\n",
    "                \"cnt_level_end_quickplay\",\n",
    "                \"cnt_level_complete_quickplay\",\n",
    "                \"cnt_level_reset_quickplay\",\n",
    "                \"cnt_post_score\",\n",
    "                \"cnt_spend_virtual_currency\",\n",
    "                \"cnt_ad_reward\",\n",
    "                \"cnt_challenge_a_friend\",\n",
    "                \"cnt_completed_5_levels\",\n",
    "                \"cnt_use_extra_steps\",\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        prediction_sample_df = pd.merge(\n",
    "            customer_aggregated_features.set_index(\"entity_id\"),\n",
    "            online_features.set_index(\"entity_id\"),\n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "        ).reset_index(drop=True)\n",
    "\n",
    "        # prediction_sample = prediction_sample_df.to_dict(\"records\")\n",
    "        prediction_instance = prediction_sample_df.values.tolist()\n",
    "        prediction = endpoint.predict(prediction_instance)\n",
    "        print(\n",
    "            f\"Prediction request: user_id - {entity_ids} - values - {prediction_instance} - prediction - {prediction[0]}\"\n",
    "        )\n",
    "        time.sleep(latency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MewDhNbrXf_N"
   },
   "source": [
    "# Setting the realtime scenario\n",
    "\n",
    "In order to make real-time churn prediction, you need to\n",
    "\n",
    "1. Collect the historical data about user's events and behaviors\n",
    "2. Design your data model, build your feature and ingest them into the Feature store to serve both offline for training and online for serving.\n",
    "3. Define churn and get the data to train a churn model\n",
    "4. Train the model at scale\n",
    "5. Deploy the model to an endpoint and generate return the prediction score in real-time\n",
    "\n",
    "You will cover those steps in details below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk,all"
   },
   "source": [
    "### Initialize Vertex AI SDK for Python\n",
    "\n",
    "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "poLJ0fV52Rrc"
   },
   "outputs": [],
   "source": [
    "vertex_ai.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ffd54e97270"
   },
   "source": [
    "### Initialize BigQuery SDK for Python\n",
    "\n",
    "Initialize the BigQuery AI SDK for Python for your project and corresponding bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "9f48872501fb"
   },
   "outputs": [],
   "source": [
    "bq_client = bigquery.Client(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WnUQO2IHC9pZ"
   },
   "source": [
    "## Identify users and build your features\n",
    " \n",
    "This section we will have static features we want to fetch from Vertex AI Feature Store. In particular, we will cover the following steps:\n",
    " \n",
    "1. Identify users, process demographic features and process behavioral features within the last 24 hours using **BigQuery**\n",
    " \n",
    "2. Set up the feature store\n",
    " \n",
    "3. Register features using **Vertex AI Feature Store** and the SDK.\n",
    " \n",
    "Below you have a picture that shows the process.\n",
    " \n",
    "<img src=\"./assets/feature_store_ingestion_2.png\">\n",
    " \n",
    " \n",
    "The original dataset contains raw event data we cannot ingest in the feature store as they are. We need to pre-process the raw data in order to get user features.\n",
    " \n",
    "**Notice we simulate those transformations in different points of time (today and tomorrow).**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9zIrwhpDF2q"
   },
   "source": [
    "### Label, Demographic and Behavioral Transformations\n",
    " \n",
    "This section is based on the [Churn prediction for game developers using Google Analytics 4 (GA4) and BigQuery ML](https://cloud.google.com/blog/topics/developers-practitioners/churn-prediction-game-developers-using-google-analytics-4-ga4-and-bigquery-ml?utm_source=linkedin&utm_medium=unpaidsoc&utm_campaign=FY21-Q2-Google-Cloud-Tech-Blog&utm_content=google-analytics-4&utm_term=-) blog article by Minhaz Kazi and Polong Lin.\n",
    " \n",
    "You will adapt it to turn a batch churn prediction (using features within the first 24h user of first engagement) into a real-time churn prediction (using features within the first 6h user of last engagement).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "RQX5m8UiC_px"
   },
   "outputs": [],
   "source": [
    "features_sql_query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE\n",
    "  `{PROJECT_ID}.{BQ_DATASET}.{FEATURES_TABLE}` AS\n",
    "WITH\n",
    "\n",
    "  # query to extract demographic data for each user ---------------------------------------------------------\n",
    "  get_demographic_data AS (\n",
    "  SELECT * EXCEPT (row_num)\n",
    "  FROM (\n",
    "    SELECT\n",
    "      user_pseudo_id,\n",
    "      geo.country as country,\n",
    "      device.operating_system as operating_system,\n",
    "      device.language as language,\n",
    "      ROW_NUMBER() OVER (PARTITION BY user_pseudo_id ORDER BY event_timestamp DESC) AS row_num\n",
    "    FROM `firebase-public-project.analytics_153293282.events_*`)\n",
    "  WHERE row_num = 1),\n",
    "\n",
    "  # query to extract behavioral data for each user ----------------------------------------------------------\n",
    "  get_behavioral_data AS (\n",
    "  SELECT\n",
    "    event_timestamp,\n",
    "    user_pseudo_id,\n",
    "    SUM(IF(event_name = 'user_engagement', 1, 0)) OVER (PARTITION BY user_pseudo_id ORDER BY event_timestamp ASC RANGE BETWEEN 21600000000 PRECEDING\n",
    "      AND CURRENT ROW ) AS cnt_user_engagement,\n",
    "    SUM(IF(event_name = 'level_start_quickplay', 1, 0)) OVER (PARTITION BY user_pseudo_id ORDER BY event_timestamp ASC RANGE BETWEEN 21600000000 PRECEDING\n",
    "      AND CURRENT ROW ) AS cnt_level_start_quickplay,\n",
    "    SUM(IF(event_name = 'level_end_quickplay', 1, 0)) OVER (PARTITION BY user_pseudo_id ORDER BY event_timestamp ASC RANGE BETWEEN 21600000000 PRECEDING\n",
    "      AND CURRENT ROW ) AS cnt_level_end_quickplay,\n",
    "    SUM(IF(event_name = 'level_complete_quickplay', 1, 0)) OVER (PARTITION BY user_pseudo_id ORDER BY event_timestamp ASC RANGE BETWEEN 21600000000 PRECEDING\n",
    "      AND CURRENT ROW ) AS cnt_level_complete_quickplay,\n",
    "    SUM(IF(event_name = 'level_reset_quickplay', 1, 0)) OVER (PARTITION BY user_pseudo_id ORDER BY event_timestamp ASC RANGE BETWEEN 21600000000 PRECEDING\n",
    "      AND CURRENT ROW ) AS cnt_level_reset_quickplay,\n",
    "    SUM(IF(event_name = 'post_score', 1, 0)) OVER (PARTITION BY user_pseudo_id ORDER BY event_timestamp ASC RANGE BETWEEN 21600000000 PRECEDING\n",
    "      AND CURRENT ROW ) AS cnt_post_score,\n",
    "    SUM(IF(event_name = 'spend_virtual_currency', 1, 0)) OVER (PARTITION BY user_pseudo_id ORDER BY event_timestamp ASC RANGE BETWEEN 21600000000 PRECEDING\n",
    "      AND CURRENT ROW ) AS cnt_spend_virtual_currency,\n",
    "    SUM(IF(event_name = 'ad_reward', 1, 0)) OVER (PARTITION BY user_pseudo_id ORDER BY event_timestamp ASC RANGE BETWEEN 21600000000 PRECEDING\n",
    "      AND CURRENT ROW ) AS cnt_ad_reward,\n",
    "    SUM(IF(event_name = 'challenge_a_friend', 1, 0)) OVER (PARTITION BY user_pseudo_id ORDER BY event_timestamp ASC RANGE BETWEEN 21600000000 PRECEDING\n",
    "      AND CURRENT ROW ) AS cnt_challenge_a_friend,\n",
    "    SUM(IF(event_name = 'completed_5_levels', 1, 0)) OVER (PARTITION BY user_pseudo_id ORDER BY event_timestamp ASC RANGE BETWEEN 21600000000 PRECEDING\n",
    "      AND CURRENT ROW ) AS cnt_completed_5_levels,\n",
    "    SUM(IF(event_name = 'use_extra_steps', 1, 0)) OVER (PARTITION BY user_pseudo_id ORDER BY event_timestamp ASC RANGE BETWEEN 21600000000 PRECEDING\n",
    "      AND CURRENT ROW ) AS cnt_use_extra_steps,\n",
    "  FROM (\n",
    "    SELECT\n",
    "      e.*\n",
    "    FROM\n",
    "      `firebase-public-project.analytics_153293282.events_*` AS e\n",
    "    )\n",
    ")\n",
    "\n",
    "SELECT\n",
    "    -- PARSE_TIMESTAMP('%Y-%m-%d %H:%M:%S', CONCAT('{TODAY}', ' ', STRING(TIME_TRUNC(CURRENT_TIME(), SECOND))), 'UTC') as timestamp,\n",
    "    TIMESTAMP_ADD(PARSE_TIMESTAMP('%Y-%m-%d %H:%M:%S', FORMAT_TIMESTAMP('%Y-%m-%d %H:%M:%S', TIMESTAMP_MICROS(beh.event_timestamp))), INTERVAL 1351 DAY) AS timestamp,\n",
    "    dem.*,\n",
    "    CAST(IFNULL(beh.cnt_user_engagement, 0) AS FLOAT64)  AS cnt_user_engagement,\n",
    "    CAST(IFNULL(beh.cnt_level_start_quickplay, 0) AS FLOAT64) AS cnt_level_start_quickplay,\n",
    "    CAST(IFNULL(beh.cnt_level_end_quickplay, 0) AS FLOAT64) AS cnt_level_end_quickplay,\n",
    "    CAST(IFNULL(beh.cnt_level_complete_quickplay, 0) AS FLOAT64) AS cnt_level_complete_quickplay,\n",
    "    CAST(IFNULL(beh.cnt_level_reset_quickplay, 0) AS FLOAT64) AS cnt_level_reset_quickplay,\n",
    "    CAST(IFNULL(beh.cnt_post_score, 0) AS FLOAT64) AS cnt_post_score,\n",
    "    CAST(IFNULL(beh.cnt_spend_virtual_currency, 0) AS FLOAT64) AS cnt_spend_virtual_currency,\n",
    "    CAST(IFNULL(beh.cnt_ad_reward, 0) AS FLOAT64) AS cnt_ad_reward,\n",
    "    CAST(IFNULL(beh.cnt_challenge_a_friend, 0) AS FLOAT64) AS cnt_challenge_a_friend,\n",
    "    CAST(IFNULL(beh.cnt_completed_5_levels, 0) AS FLOAT64) AS cnt_completed_5_levels,\n",
    "    CAST(IFNULL(beh.cnt_use_extra_steps, 0) AS FLOAT64) AS cnt_use_extra_steps,\n",
    "FROM\n",
    "  get_demographic_data dem\n",
    "LEFT OUTER JOIN \n",
    "  get_behavioral_data beh\n",
    "ON\n",
    "  dem.user_pseudo_id = beh.user_pseudo_id\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "oGYxLCSnD068"
   },
   "outputs": [],
   "source": [
    "run_bq_query(features_sql_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQLIlsTCD_nk"
   },
   "source": [
    "## Create a Vertex AI Feature store and ingest your features\n",
    " \n",
    "Now you have a wide table of features. It is time to ingest them into the feature store.\n",
    " \n",
    "Before to moving on, you may have a question: **Why do I need a feature store**\n",
    "in this scenario at that point?\n",
    " \n",
    "One of the reasons would be to make those features accessible across teams by calculating once and reuse them many times. And in order to make it possible you need also be able to monitor those features over time to guarantee freshness and in case have a new feature engineering run to refresh them.\n",
    " \n",
    "If it is not your case, I will give even more reasons about why you should consider a feature store in the following sections. Just keep following me for now.\n",
    " \n",
    "One of the most important things is related to its data model. As you can see in the picture below, Vertex AI Feature Store organizes resources hierarchically in the following order: `Featurestore -> EntityType -> Feature`. You must create these resources before you can ingest data into Vertex AI Feature Store.\n",
    " \n",
    "<img src=\"./assets/feature_store_data_model_3.png\">\n",
    " \n",
    "In our case we are going to create **mobile_gaming** featurestore resource containing **user** entity type and all its associated **features** such as country or the number of times a user challenged a friend (cnt_challenge_a_friend).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VR7BJEozED_Q"
   },
   "source": [
    "### Create featurestore, ```mobile_gaming```\n",
    "\n",
    "You need to create a `featurestore` resource to contain entity types, features, and feature values. In your case, you would call it `mobile_gaming`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "vUFqtYU-EDTR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Featurestore\n",
      "Create Featurestore backing LRO: projects/106131389347/locations/us-central1/featurestores/mobile_gaming/operations/6505002504101560320\n",
      "Featurestore created. Resource name: projects/106131389347/locations/us-central1/featurestores/mobile_gaming\n",
      "To use this Featurestore in another session:\n",
      "featurestore = aiplatform.Featurestore('projects/106131389347/locations/us-central1/featurestores/mobile_gaming')\n",
      "Feature store created: projects/106131389347/locations/us-central1/featurestores/mobile_gaming\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    mobile_gaming_feature_store = Featurestore.create(\n",
    "        featurestore_id=FEATURESTORE_ID,\n",
    "        online_store_fixed_node_count=ONLINE_STORE_NODES_COUNT,\n",
    "        labels={\"team\": \"dataoffice\", \"app\": \"mobile_gaming\"},\n",
    "        sync=True,\n",
    "    )\n",
    "except RuntimeError as error:\n",
    "    print(error)\n",
    "else:\n",
    "    FEATURESTORE_RESOURCE_NAME = mobile_gaming_feature_store.resource_name\n",
    "    print(f\"Feature store created: {FEATURESTORE_RESOURCE_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mUlCwfdpEHJG"
   },
   "source": [
    "### Create the ```User``` entity type and its features\n",
    "\n",
    "You define your own entity types which represents one or more level you decide to refer your features. In your case, it would have a `user` entity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "PnCU1wBND3W7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating EntityType\n",
      "Create EntityType backing LRO: projects/106131389347/locations/us-central1/featurestores/mobile_gaming/entityTypes/user/operations/3807346327306633216\n",
      "EntityType created. Resource name: projects/106131389347/locations/us-central1/featurestores/mobile_gaming/entityTypes/user\n",
      "To use this EntityType in another session:\n",
      "entity_type = aiplatform.EntityType('projects/106131389347/locations/us-central1/featurestores/mobile_gaming/entityTypes/user')\n",
      "Entity type name is projects/106131389347/locations/us-central1/featurestores/mobile_gaming/entityTypes/user\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    user_entity_type = mobile_gaming_feature_store.create_entity_type(\n",
    "        entity_type_id=ENTITY_ID, description=\"User Entity\", sync=True\n",
    "    )\n",
    "except RuntimeError as error:\n",
    "    print(error)\n",
    "else:\n",
    "    USER_ENTITY_RESOURCE_NAME = user_entity_type.resource_name\n",
    "    print(\"Entity type name is\", USER_ENTITY_RESOURCE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bT9LXzu1EOvW"
   },
   "source": [
    "### Set Feature Monitoring\n",
    "\n",
    "Notice that Vertex AI Feature store has [feature monitoring capability](https://cloud.google.com/vertex-ai/docs/featurestore/monitoring). It is in preview, so you need to use v1beta1 Python which is a lower-level API than the one we've used so far in this notebook. \n",
    "\n",
    "The easiest way to set this for now is using [console UI](https://console.cloud.google.com/vertex-ai/features). For completeness, below is example to do this using v1beta1 SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "8WBlYUkOERaI"
   },
   "outputs": [],
   "source": [
    "from google.cloud.aiplatform_v1beta1 import \\\n",
    "    FeaturestoreServiceClient as v1beta1_FeaturestoreServiceClient\n",
    "from google.cloud.aiplatform_v1beta1.types import \\\n",
    "    entity_type as v1beta1_entity_type_pb2\n",
    "from google.cloud.aiplatform_v1beta1.types import \\\n",
    "    featurestore_monitoring as v1beta1_featurestore_monitoring_pb2\n",
    "from google.cloud.aiplatform_v1beta1.types import \\\n",
    "    featurestore_service as v1beta1_featurestore_service_pb2\n",
    "from google.protobuf.duration_pb2 import Duration\n",
    "\n",
    "v1beta1_admin_client = v1beta1_FeaturestoreServiceClient(\n",
    "    client_options={\"api_endpoint\": API_ENDPOINT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "92X4-7PFETj5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"projects/106131389347/locations/us-central1/featurestores/mobile_gaming/entityTypes/user\"\n",
       "description: \"User Entity\"\n",
       "create_time {\n",
       "  seconds: 1667820617\n",
       "  nanos: 44040000\n",
       "}\n",
       "update_time {\n",
       "  seconds: 1667820620\n",
       "  nanos: 899901000\n",
       "}\n",
       "etag: \"AMEw9yMCAwtakwK4lrBDTGp1qK2IOJCc5HMCtL1-l6mVi8Cg37ZTfo6PxLS0ucRIygOh\"\n",
       "monitoring_config {\n",
       "  snapshot_analysis {\n",
       "    monitoring_interval {\n",
       "      seconds: 86400\n",
       "    }\n",
       "    monitoring_interval_days: 1\n",
       "    staleness_days: 21\n",
       "  }\n",
       "  numerical_threshold_config {\n",
       "    value: 0.3\n",
       "  }\n",
       "  categorical_threshold_config {\n",
       "    value: 0.3\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1beta1_admin_client.update_entity_type(\n",
    "    v1beta1_featurestore_service_pb2.UpdateEntityTypeRequest(\n",
    "        entity_type=v1beta1_entity_type_pb2.EntityType(\n",
    "            name=v1beta1_admin_client.entity_type_path(\n",
    "                PROJECT_ID, REGION, FEATURESTORE_ID, ENTITY_ID\n",
    "            ),\n",
    "            monitoring_config=v1beta1_featurestore_monitoring_pb2.FeaturestoreMonitoringConfig(\n",
    "                snapshot_analysis=v1beta1_featurestore_monitoring_pb2.FeaturestoreMonitoringConfig.SnapshotAnalysis(\n",
    "                    monitoring_interval=Duration(seconds=86400),  # 1 day\n",
    "                ),\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxAuZjt3EWFo"
   },
   "source": [
    "### Create features\n",
    "\n",
    "In order to ingest features, you need to provide feature configuration and create them as featurestore resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hRXO2I5VEYwt"
   },
   "source": [
    "#### Create Feature configuration\n",
    "\n",
    "For simplicity, I created the configuration in a declarative way. Of course, we can create an helper function to built it from Bigquery schema.\n",
    "Also notice that we want to pass some feature on-fly. In this case, it country, operating system and language looks perfect for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "K26NEYZIEbvE"
   },
   "outputs": [],
   "source": [
    "feature_configs = {\n",
    "    \"country\": {\n",
    "        \"value_type\": \"STRING\",\n",
    "        \"description\": \"The country of customer\",\n",
    "        \"labels\": {\"status\": \"passed\"},\n",
    "    },\n",
    "    \"operating_system\": {\n",
    "        \"value_type\": \"STRING\",\n",
    "        \"description\": \"The operating system of device\",\n",
    "        \"labels\": {\"status\": \"passed\"},\n",
    "    },\n",
    "    \"language\": {\n",
    "        \"value_type\": \"STRING\",\n",
    "        \"description\": \"The language of device\",\n",
    "        \"labels\": {\"status\": \"passed\"},\n",
    "    },\n",
    "    \"cnt_user_engagement\": {\n",
    "        \"value_type\": \"DOUBLE\",\n",
    "        \"description\": \"A variable of user engagement level\",\n",
    "        \"labels\": {\"status\": \"passed\"},\n",
    "    },\n",
    "    \"cnt_level_start_quickplay\": {\n",
    "        \"value_type\": \"DOUBLE\",\n",
    "        \"description\": \"A variable of user engagement with start level\",\n",
    "        \"labels\": {\"status\": \"passed\"},\n",
    "    },\n",
    "    \"cnt_level_end_quickplay\": {\n",
    "        \"value_type\": \"DOUBLE\",\n",
    "        \"description\": \"A variable of user engagement with end level\",\n",
    "        \"labels\": {\"status\": \"passed\"},\n",
    "    },\n",
    "    \"cnt_level_complete_quickplay\": {\n",
    "        \"value_type\": \"DOUBLE\",\n",
    "        \"description\": \"A variable of user engagement with complete status\",\n",
    "        \"labels\": {\"status\": \"passed\"},\n",
    "    },\n",
    "    \"cnt_level_reset_quickplay\": {\n",
    "        \"value_type\": \"DOUBLE\",\n",
    "        \"description\": \"A variable of user engagement with reset status\",\n",
    "        \"labels\": {\"status\": \"passed\"},\n",
    "    },\n",
    "    \"cnt_post_score\": {\n",
    "        \"value_type\": \"DOUBLE\",\n",
    "        \"description\": \"A variable of user score\",\n",
    "        \"labels\": {\"status\": \"passed\"},\n",
    "    },\n",
    "    \"cnt_spend_virtual_currency\": {\n",
    "        \"value_type\": \"DOUBLE\",\n",
    "        \"description\": \"A variable of user virtual amount\",\n",
    "        \"labels\": {\"status\": \"passed\"},\n",
    "    },\n",
    "    \"cnt_ad_reward\": {\n",
    "        \"value_type\": \"DOUBLE\",\n",
    "        \"description\": \"A variable of user reward\",\n",
    "        \"labels\": {\"status\": \"passed\"},\n",
    "    },\n",
    "    \"cnt_challenge_a_friend\": {\n",
    "        \"value_type\": \"DOUBLE\",\n",
    "        \"description\": \"A variable of user challenges with friends\",\n",
    "        \"labels\": {\"status\": \"passed\"},\n",
    "    },\n",
    "    \"cnt_completed_5_levels\": {\n",
    "        \"value_type\": \"DOUBLE\",\n",
    "        \"description\": \"A variable of user level 5 completed\",\n",
    "        \"labels\": {\"status\": \"passed\"},\n",
    "    },\n",
    "    \"cnt_use_extra_steps\": {\n",
    "        \"value_type\": \"DOUBLE\",\n",
    "        \"description\": \"A variable of user extra steps\",\n",
    "        \"labels\": {\"status\": \"passed\"},\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FjzMd1XbEfdo"
   },
   "source": [
    "#### Create features using `batch_create_features` method\n",
    "\n",
    "Once you have the feature configuration, you can create feature resources using `batch_create_features` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "nqlgCDI9pbCD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch creating features EntityType entityType: projects/106131389347/locations/us-central1/featurestores/mobile_gaming/entityTypes/user\n",
      "Batch create Features EntityType entityType backing LRO: projects/106131389347/locations/us-central1/featurestores/mobile_gaming/operations/1959744580177887232\n",
      "EntityType entityType Batch created features. Resource name: projects/106131389347/locations/us-central1/featurestores/mobile_gaming/entityTypes/user\n",
      "\n",
      "The resource name of cnt_level_end_quickplay feature is projects/106131389347/locations/us-central1/featurestores/mobile_gaming/entityTypes/user/features/cnt_level_end_quickplay\n",
      "\n",
      "The resource name of cnt_use_extra_steps feature is projects/106131389347/locations/us-central1/featurestores/mobile_gaming/entityTypes/user/features/cnt_use_extra_steps\n",
      "\n",
      "The resource name of cnt_spend_virtual_currency feature is projects/106131389347/locations/us-central1/featurestores/mobile_gaming/entityTypes/user/features/cnt_spend_virtual_currency\n",
      "\n",
      "The resource name of operating_system feature is projects/106131389347/locations/us-central1/featurestores/mobile_gaming/entityTypes/user/features/operating_system\n",
      "\n",
      "The resource name of cnt_level_reset_quickplay feature is projects/106131389347/locations/us-central1/featurestores/mobile_gaming/entityTypes/user/features/cnt_level_reset_quickplay\n",
      "\n",
      "The resource name of cnt_challenge_a_friend feature is projects/106131389347/locations/us-central1/featurestores/mobile_gaming/entityTypes/user/features/cnt_challenge_a_friend\n",
      "\n",
      "The resource name of cnt_user_engagement feature is projects/106131389347/locations/us-central1/featurestores/mobile_gaming/entityTypes/user/features/cnt_user_engagement\n",
      "\n",
      "The resource name of cnt_level_complete_quickplay feature is projects/106131389347/locations/us-central1/featurestores/mobile_gaming/entityTypes/user/features/cnt_level_complete_quickplay\n",
      "\n",
      "The resource name of cnt_ad_reward feature is projects/106131389347/locations/us-central1/featurestores/mobile_gaming/entityTypes/user/features/cnt_ad_reward\n",
      "\n",
      "The resource name of language feature is projects/106131389347/locations/us-central1/featurestores/mobile_gaming/entityTypes/user/features/language\n",
      "\n",
      "The resource name of cnt_post_score feature is projects/106131389347/locations/us-central1/featurestores/mobile_gaming/entityTypes/user/features/cnt_post_score\n",
      "\n",
      "The resource name of country feature is projects/106131389347/locations/us-central1/featurestores/mobile_gaming/entityTypes/user/features/country\n",
      "\n",
      "The resource name of cnt_completed_5_levels feature is projects/106131389347/locations/us-central1/featurestores/mobile_gaming/entityTypes/user/features/cnt_completed_5_levels\n",
      "\n",
      "The resource name of cnt_level_start_quickplay feature is projects/106131389347/locations/us-central1/featurestores/mobile_gaming/entityTypes/user/features/cnt_level_start_quickplay\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    user_entity_type.batch_create_features(feature_configs=feature_configs, sync=True)\n",
    "except RuntimeError as error:\n",
    "    print(error)\n",
    "else:\n",
    "    for feature in user_entity_type.list_features():\n",
    "        print(\"\")\n",
    "        print(f\"The resource name of {feature.name} feature is\", feature.resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7zpFV7wAppkC"
   },
   "source": [
    "### Search features\n",
    "\n",
    "Vertex AI Feature store supports searching capabilities. Below you have a simple example that shows how to filter a feature based on its name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "BJXYLLOfppCL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<google.cloud.aiplatform.featurestore.feature.Feature object at 0x7f53bee20850> \n",
       " resource name: projects/106131389347/locations/us-central1/featurestores/mobile_gaming/entityTypes/user/features/cnt_user_engagement]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_query = \"feature_id:cnt_user_engagement\"\n",
    "searched_features = Feature.search(query=feature_query)\n",
    "searched_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "is9C_6-QpxG3"
   },
   "source": [
    "## Ingest features \n",
    "\n",
    "At that point, you create all resources associated to the feature store. You just need to import feature values before you can use them for online/offline serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "4QgjW2vhsEUE"
   },
   "outputs": [],
   "source": [
    "FEATURES_IDS = [feature.name for feature in user_entity_type.list_features()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "U5wE5h4GN_aA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing EntityType feature values: projects/106131389347/locations/us-central1/featurestores/mobile_gaming/entityTypes/user\n",
      "Import EntityType feature values backing LRO: projects/106131389347/locations/us-central1/featurestores/mobile_gaming/entityTypes/user/operations/5954437449655517184\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    user_entity_type.ingest_from_bq(\n",
    "        feature_ids=FEATURES_IDS,\n",
    "        feature_time=FEATURE_TIME,\n",
    "        bq_source_uri=BQ_SOURCE_URI,\n",
    "        entity_id_field=ENTITY_ID_FIELD,\n",
    "        disable_online_serving=False,\n",
    "        worker_count=10,\n",
    "        sync=False,\n",
    "    )\n",
    "except RuntimeError as error:\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8lCMpDPGp-oQ"
   },
   "source": [
    "# Train and deploy a real-time churn ML model using Vertex AI Training and Endpoints\n",
    "\n",
    "Now that you have your features and you are almost ready to train our churn model.\n",
    "\n",
    "Below an high level picture\n",
    "\n",
    "<img src=\"./train_model_4.png\">\n",
    "\n",
    "Let's dive into each step of this process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VMrvnuyjqGfY"
   },
   "source": [
    "## Fetch training data with point-in-time query using BigQuery and Vertex AI Feature store \n",
    " \n",
    "As we mentioned above, in real time churn prediction, it is so important defining the label you want to predict with your model.\n",
    " \n",
    "Let's assume that you decide to predict the churn probability over the next hour. So now you have your label. Next step is to define your training sample. But let's think about that for a second.\n",
    " \n",
    "In that churn real time system, you have a high volume of transactions you could use to calculate those features which keep floating and are collected constantly over time. It implies that you always get fresh data to reconstruct features. And depending on when you decide to calculate one feature or another you can end up with a set of features that are not aligned in time.\n",
    " \n",
    "When you have labels available, it would be incredibly difficult to say which set of features contains the most up to date historical information associated with the label you want to predict. And, when you are not able to guarantee that, the performance of your model would be badly affected because you serve no representative features of the data and the label from the field when it goes live. So you need a way to get the most updated features you calculated over time before the label becomes available in order to avoid this informational skew.\n",
    " \n",
    "**With the Vertex AI Feature store, you can fetch feature values corresponding to a particular timestamp thanks to point-in-time lookup capability.** In our case, it would be the timestamp associated with the label you want to predict with your model. In this way, you will avoid data leakage and you will get the most updated features to train your model.\n",
    " \n",
    "Let's see how to do that.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RE_Pvmu-qdDt"
   },
   "source": [
    "### Define query for reading instances at a specific point in time\n",
    "\n",
    "First thing, you need to define the set of reading instances at a specific point in time you want to consider in order to generate your training sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "bUDVw7l-qF2x"
   },
   "outputs": [],
   "source": [
    "read_instances_query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE\n",
    "  `{PROJECT_ID}.{BQ_DATASET}.{READ_INSTANCES_TABLE}` AS\n",
    "WITH\n",
    "\n",
    "  # get training threshold ----------------------------------------------------------------------------------\n",
    "  get_training_threshold AS (\n",
    "  SELECT\n",
    "    (MAX(event_timestamp) - 10800000000) AS training_thrs\n",
    "  FROM\n",
    "    `firebase-public-project.analytics_153293282.events_*`\n",
    "  WHERE\n",
    "    event_name=\"user_engagement\"\n",
    "    AND\n",
    "    TIMESTAMP_ADD(PARSE_TIMESTAMP('%Y-%m-%d %H:%M:%S', FORMAT_TIMESTAMP('%Y-%m-%d %H:%M:%S', TIMESTAMP_MICROS(event_timestamp))), INTERVAL 1351 DAY) < '{TODAY}'),\n",
    "\n",
    "  # query to create label -----------------------------------------------------------------------------------\n",
    "  get_label AS (\n",
    "  SELECT\n",
    "    user_pseudo_id,\n",
    "    user_last_engagement,\n",
    "    #label = 1 if last_touch within last hour hr else 0\n",
    "  IF\n",
    "    (user_last_engagement < (\n",
    "      SELECT\n",
    "        training_thrs\n",
    "      FROM\n",
    "        get_training_threshold),\n",
    "      1,\n",
    "      0 ) AS churned\n",
    "  FROM (\n",
    "    SELECT\n",
    "      user_pseudo_id,\n",
    "      MAX(event_timestamp) AS user_last_engagement\n",
    "    FROM\n",
    "      `firebase-public-project.analytics_153293282.events_*`\n",
    "    WHERE\n",
    "      event_name=\"user_engagement\"\n",
    "    AND\n",
    "    TIMESTAMP_ADD(PARSE_TIMESTAMP('%Y-%m-%d %H:%M:%S', FORMAT_TIMESTAMP('%Y-%m-%d %H:%M:%S', TIMESTAMP_MICROS(event_timestamp))), INTERVAL 1351 DAY) < '{TODAY}'\n",
    "    GROUP BY\n",
    "      user_pseudo_id )\n",
    "  GROUP BY\n",
    "    1,\n",
    "    2),\n",
    "\n",
    "  # query to create class weights --------------------------------------------------------------------------------\n",
    "  get_class_weights AS (\n",
    "  SELECT\n",
    "    CAST(COUNT(*) / (2*(COUNT(*) - SUM(churned))) AS STRING) AS class_weight_zero,\n",
    "    CAST(COUNT(*) / (2*SUM(churned)) AS STRING) AS class_weight_one,\n",
    "  FROM\n",
    "    get_label )\n",
    "\n",
    "SELECT\n",
    "  user_pseudo_id as user,\n",
    "  PARSE_TIMESTAMP('%Y-%m-%d %H:%M:%S', CONCAT('{TODAY}', ' ', STRING(TIME_TRUNC(CURRENT_TIME(), SECOND))), 'UTC') as timestamp,\n",
    "  churned AS churned,\n",
    "  CASE\n",
    "      WHEN churned = 0 THEN ( SELECT class_weight_zero FROM get_class_weights)\n",
    "      ELSE ( SELECT class_weight_one\n",
    "       FROM get_class_weights)\n",
    "    END AS class_weights\n",
    "FROM\n",
    "  get_label \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTB3853wcYm6"
   },
   "source": [
    "### Create the BigQuery instances tables\n",
    "\n",
    "You store those instances in a Bigquery table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "0PRgsZl09vJ0"
   },
   "outputs": [],
   "source": [
    "run_bq_query(read_instances_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SwSsdk2AcdTf"
   },
   "source": [
    "### Serve features for batch training\n",
    "\n",
    "Then you use the `batch_serve_to_gcs` in order to generate your training sample and store it as csv file in a target cloud bucket.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "LnP0Q5XtwfYM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving Featurestore feature values: projects/106131389347/locations/us-central1/featurestores/mobile_gaming\n",
      "Serve Featurestore feature values backing LRO: projects/106131389347/locations/us-central1/featurestores/mobile_gaming/operations/7348864484280107008\n",
      "Featurestore feature values served. Resource name: projects/106131389347/locations/us-central1/featurestores/mobile_gaming\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.featurestore.featurestore.Featurestore object at 0x7f53ab3127d0> \n",
       "resource name: projects/106131389347/locations/us-central1/featurestores/mobile_gaming"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mobile_gaming_feature_store.batch_serve_to_gcs(\n",
    "    gcs_destination_output_uri_prefix=GCS_DESTINATION_OUTPUT_URI,\n",
    "    gcs_destination_type=\"csv\",\n",
    "    serving_feature_ids=SERVING_FEATURE_IDS,\n",
    "    read_instances_uri=READ_INSTANCES_URI,\n",
    "    pass_through_fields=[\"churned\", \"class_weights\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vjvv7NKnYfcU"
   },
   "source": [
    "## Train a custom model on Vertex AI with Training Pipelines\n",
    "\n",
    "Now that we produce the training sample, we use the Vertex AI SDK to train an new version of the model using Vertex AI Training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gm8PKhFfoHO4"
   },
   "source": [
    "### Create training package and training sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "zAYW_N_Ewc_5"
   },
   "outputs": [],
   "source": [
    "!rm -Rf train_package #if train_package already exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "OZLVGnWzBVER"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://vertex-experiments-dl/data/features/train_features_20220616/000000000000.csv...\n",
      "/ [1/1 files][  1.2 MiB/  1.2 MiB] 100% Done                                    \n",
      "Operation completed over 1 objects/1.2 MiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!mkdir -m 777 -p trainer data/ingest data/raw model config\n",
    "!gsutil -m cp -r $GCS_DESTINATION_OUTPUT_URI/*.csv data/ingest\n",
    "!head -n 2000 data/ingest/*.csv > data/raw/sample.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZLmh54sm8m0l"
   },
   "source": [
    "### Create training script\n",
    "\n",
    "You create the training script to train a XGboost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "oTp85Qfpwc_6"
   },
   "outputs": [],
   "source": [
    "!touch trainer/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "PVGEf68fDSax"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile trainer/task.py\n",
    "import os\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import yaml\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def get_args():\n",
    "    \"\"\"\n",
    "    Get arguments from command line.\n",
    "    Returns:\n",
    "        args: parsed arguments\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--data_path',\n",
    "        required=False,\n",
    "        default=os.getenv('AIP_TRAINING_DATA_URI'),\n",
    "        type=str,\n",
    "        help='path to read data')\n",
    "    parser.add_argument(\n",
    "        '--learning_rate',\n",
    "        required=False,\n",
    "        default=0.01,\n",
    "        type=int,\n",
    "        help='number of epochs')\n",
    "    parser.add_argument(\n",
    "        '--model_dir',\n",
    "        required=False,\n",
    "        default=os.getenv('AIP_MODEL_DIR'),\n",
    "        type=str,\n",
    "        help='dir to store saved model')\n",
    "    parser.add_argument(\n",
    "        '--config_path',\n",
    "        required=False,\n",
    "        default='../config.yaml',\n",
    "        type=str,\n",
    "        help='path to read config file')\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def ingest_data(data_path, data_model_params):\n",
    "    \"\"\"\n",
    "    Ingest data\n",
    "    Args:\n",
    "        data_path: path to read data\n",
    "        data_model_params: data model parameters\n",
    "    Returns:\n",
    "        df: dataframe\n",
    "    \"\"\"\n",
    "    # read training data\n",
    "    df = pd.read_csv(data_path, sep=',',\n",
    "                     dtype={col: 'string' for col in data_model_params['categorical_features']})\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess_data(df, data_model_params):\n",
    "    \"\"\"\n",
    "    Preprocess data\n",
    "    Args:\n",
    "        df: dataframe\n",
    "        data_model_params: data model parameters\n",
    "    Returns:\n",
    "        df: dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    # convert nan values because pd.NA ia not supported by SimpleImputer\n",
    "    # bug in sklearn 0.23.1 version: https://github.com/scikit-learn/scikit-learn/pull/17526\n",
    "    # decided to skip NAN values for now\n",
    "    df.replace({pd.NA: np.nan}, inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # get features and labels\n",
    "    x = df[data_model_params['numerical_features'] + data_model_params['categorical_features'] + [\n",
    "        data_model_params['weight_feature']]]\n",
    "    y = df[data_model_params['target']]\n",
    "\n",
    "    # train-test split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y,\n",
    "                                                        test_size=data_model_params['train_test_split']['test_size'],\n",
    "                                                        random_state=data_model_params['train_test_split'][\n",
    "                                                            'random_state'])\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n",
    "def build_pipeline(learning_rate, model_params):\n",
    "    \"\"\"\n",
    "    Build pipeline\n",
    "    Args:\n",
    "        learning_rate: learning rate\n",
    "        model_params: model parameters\n",
    "    Returns:\n",
    "        pipeline: pipeline\n",
    "    \"\"\"\n",
    "    # build pipeline\n",
    "    pipeline = Pipeline([\n",
    "        # ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('encoder', OneHotEncoder(handle_unknown='ignore')),\n",
    "        ('model', xgb.XGBClassifier(learning_rate=learning_rate,\n",
    "                                    use_label_encoder=False, #deprecated and breaks Vertex AI predictions\n",
    "                                    **model_params))\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "def main():\n",
    "    print('Starting training...')\n",
    "    args = get_args()\n",
    "    data_path = args.data_path\n",
    "    learning_rate = args.learning_rate\n",
    "    model_dir = args.model_dir\n",
    "    config_path = args.config_path\n",
    "\n",
    "    # read config file\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    f.close()\n",
    "    data_model_params = config['data_model_params']\n",
    "    model_params = config['model_params']\n",
    "\n",
    "    # ingest data\n",
    "    print('Reading data...')\n",
    "    data_df = ingest_data(data_path, data_model_params)\n",
    "\n",
    "    # preprocess data\n",
    "    print('Preprocessing data...')\n",
    "    x_train, x_test, y_train, y_test = preprocess_data(data_df, data_model_params)\n",
    "    sample_weight = x_train.pop(data_model_params['weight_feature'])\n",
    "    sample_weight_eval_set = x_test.pop(data_model_params['weight_feature'])\n",
    "\n",
    "    # train lgb model\n",
    "    print('Training model...')\n",
    "    xgb_pipeline = build_pipeline(learning_rate, model_params)\n",
    "    # need to use fit_transform to get the encoded eval data\n",
    "    x_train_transformed = xgb_pipeline[:-1].fit_transform(x_train)\n",
    "    x_test_transformed = xgb_pipeline[:-1].transform(x_test)\n",
    "    xgb_pipeline[-1].fit(x_train_transformed, y_train,\n",
    "                         sample_weight=sample_weight,\n",
    "                         eval_set=[(x_test_transformed, y_test)],\n",
    "                         sample_weight_eval_set=[sample_weight_eval_set],\n",
    "                         eval_metric='error',\n",
    "                         early_stopping_rounds=50,\n",
    "                         verbose=True)\n",
    "    # save model\n",
    "    print('Saving model...')\n",
    "    model_path = Path(model_dir)\n",
    "    model_path.mkdir(parents=True, exist_ok=True)\n",
    "    joblib.dump(xgb_pipeline, f'{model_dir}/model.joblib')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LD28QoIw8sdf"
   },
   "source": [
    "### Create requirements.txt\n",
    "\n",
    "You write the requirement file to build the training container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "CZfnfDAH8yos"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "pip==22.0.4\n",
    "PyYAML==5.3.1\n",
    "joblib==0.15.1\n",
    "numpy==1.18.5\n",
    "pandas==1.0.4\n",
    "scipy==1.4.1\n",
    "scikit-learn==0.23.1\n",
    "xgboost==1.1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F3KLInhPwc_7"
   },
   "source": [
    "### Create training configuration\n",
    "\n",
    "You create a training configuration with data and model params. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "_E7dPiChwc_7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing config/config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile config/config.yaml\n",
    "data_model_params:\n",
    "  target: churned\n",
    "  categorical_features:\n",
    "    - country\n",
    "    - operating_system\n",
    "    - language\n",
    "  numerical_features:\n",
    "    - cnt_user_engagement\n",
    "    - cnt_level_start_quickplay\n",
    "    - cnt_level_end_quickplay\n",
    "    - cnt_level_complete_quickplay\n",
    "    - cnt_level_reset_quickplay\n",
    "    - cnt_post_score\n",
    "    - cnt_spend_virtual_currency\n",
    "    - cnt_ad_reward\n",
    "    - cnt_challenge_a_friend\n",
    "    - cnt_completed_5_levels\n",
    "    - cnt_use_extra_steps\n",
    "  weight_feature: class_weights\n",
    "  train_test_split:\n",
    "    test_size: 0.2\n",
    "    random_state: 8\n",
    "model_params:\n",
    "  booster: gbtree\n",
    "  objective: binary:logistic\n",
    "  max_depth: 80\n",
    "  n_estimators: 100\n",
    "  random_state: 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55WduJ3M9WuH"
   },
   "source": [
    "### Test the model locally with `local-run`\n",
    "\n",
    "You leverage the Vertex AI SDK `local-run` to test the script locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "TKMtFOTM9R63"
   },
   "outputs": [],
   "source": [
    "test_job_script = f\"\"\"\n",
    "gcloud ai custom-jobs local-run \\\n",
    "--executor-image-uri={BASE_CPU_IMAGE} \\\n",
    "--python-module=trainer.task \\\n",
    "--extra-dirs=config,data,model \\\n",
    "-- \\\n",
    "--data_path data/raw/sample.csv \\\n",
    "--model_dir model \\\n",
    "--config_path config/config.yaml\n",
    "\"\"\"\n",
    "\n",
    "with open(\"local_train_job_run.sh\", \"w+\") as s:\n",
    "    s.write(test_job_script)\n",
    "s.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "9teD2VCAwc_7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package is set to /home/jupyter/Exercises/03_Feature Store.\n",
      "/usr/lib/google-cloud-sdk/platform/bundledpythonunix/lib/python3.9/subprocess.py:935: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdin = io.open(p2cwrite, 'wb', bufsize)\n",
      "/usr/lib/google-cloud-sdk/platform/bundledpythonunix/lib/python3.9/subprocess.py:941: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\n",
      "Sending build context to Docker daemon  2.583MB\n",
      "Step 1/13 : FROM us-docker.pkg.dev/vertex-ai/training/scikit-learn-cpu.0-23:latest\n",
      "latest: Pulling from vertex-ai/training/scikit-learn-cpu.0-23\n",
      "2c11b7cecaa5: Pulling fs layer\n",
      "04637fa56252: Pulling fs layer\n",
      "d6e6af23a0f3: Pulling fs layer\n",
      "b4a424de92ad: Pulling fs layer\n",
      "59fdb7d45b6c: Pulling fs layer\n",
      "7759500808dd: Pulling fs layer\n",
      "89fa8d1cd3c8: Pulling fs layer\n",
      "41c3544d11de: Pulling fs layer\n",
      "32634d002d01: Pulling fs layer\n",
      "3d8caa4d24d5: Pulling fs layer\n",
      "0b38311ee46d: Pulling fs layer\n",
      "36e73f53893e: Pulling fs layer\n",
      "d27528dc2d9f: Pulling fs layer\n",
      "2ca38bff7b4b: Pulling fs layer\n",
      "59fdb7d45b6c: Waiting\n",
      "29731ba486cc: Pulling fs layer\n",
      "266a99828791: Pulling fs layer\n",
      "7759500808dd: Waiting\n",
      "ab02fbc06098: Pulling fs layer\n",
      "89fa8d1cd3c8: Waiting\n",
      "090a178c469c: Pulling fs layer\n",
      "34c760d06da9: Pulling fs layer\n",
      "41c3544d11de: Waiting\n",
      "07e648356d7b: Pulling fs layer\n",
      "5eb92d55fd4b: Pulling fs layer\n",
      "ebfd527d91dd: Pulling fs layer\n",
      "cd9320933d55: Pulling fs layer\n",
      "6bd7c5ee4c13: Pulling fs layer\n",
      "a033a60349a3: Pulling fs layer\n",
      "10464939f218: Pulling fs layer\n",
      "a3320505acfb: Pulling fs layer\n",
      "c478ef85c719: Pulling fs layer\n",
      "a47ff4dd1721: Pulling fs layer\n",
      "7287905e7101: Pulling fs layer\n",
      "b38df7065dd7: Pulling fs layer\n",
      "e1358f07d66b: Pulling fs layer\n",
      "b47d8fe99d7a: Pulling fs layer\n",
      "a5faaecb2165: Pulling fs layer\n",
      "a14804e0781b: Pulling fs layer\n",
      "cb5b6ad7922b: Pulling fs layer\n",
      "e171fd9dd068: Pulling fs layer\n",
      "c154e275cfdf: Pulling fs layer\n",
      "cb32ad90adc6: Pulling fs layer\n",
      "6bf34d10d7a4: Pulling fs layer\n",
      "32634d002d01: Waiting\n",
      "3d8caa4d24d5: Waiting\n",
      "0b38311ee46d: Waiting\n",
      "36e73f53893e: Waiting\n",
      "d27528dc2d9f: Waiting\n",
      "2ca38bff7b4b: Waiting\n",
      "29731ba486cc: Waiting\n",
      "266a99828791: Waiting\n",
      "ab02fbc06098: Waiting\n",
      "090a178c469c: Waiting\n",
      "e1358f07d66b: Waiting\n",
      "34c760d06da9: Waiting\n",
      "07e648356d7b: Waiting\n",
      "b47d8fe99d7a: Waiting\n",
      "5eb92d55fd4b: Waiting\n",
      "a3320505acfb: Waiting\n",
      "a5faaecb2165: Waiting\n",
      "c478ef85c719: Waiting\n",
      "a14804e0781b: Waiting\n",
      "ebfd527d91dd: Waiting\n",
      "a47ff4dd1721: Waiting\n",
      "cb5b6ad7922b: Waiting\n",
      "cd9320933d55: Waiting\n",
      "7287905e7101: Waiting\n",
      "6bd7c5ee4c13: Waiting\n",
      "b38df7065dd7: Waiting\n",
      "e171fd9dd068: Waiting\n",
      "a033a60349a3: Waiting\n",
      "c154e275cfdf: Waiting\n",
      "10464939f218: Waiting\n",
      "cb32ad90adc6: Waiting\n",
      "6bf34d10d7a4: Waiting\n",
      "b4a424de92ad: Waiting\n",
      "d6e6af23a0f3: Verifying Checksum\n",
      "d6e6af23a0f3: Download complete\n",
      "04637fa56252: Verifying Checksum\n",
      "04637fa56252: Download complete\n",
      "b4a424de92ad: Verifying Checksum\n",
      "b4a424de92ad: Download complete\n",
      "59fdb7d45b6c: Verifying Checksum\n",
      "59fdb7d45b6c: Download complete\n",
      "7759500808dd: Download complete\n",
      "89fa8d1cd3c8: Verifying Checksum\n",
      "89fa8d1cd3c8: Download complete\n",
      "32634d002d01: Download complete\n",
      "2c11b7cecaa5: Verifying Checksum\n",
      "2c11b7cecaa5: Download complete\n",
      "3d8caa4d24d5: Verifying Checksum\n",
      "3d8caa4d24d5: Download complete\n",
      "0b38311ee46d: Verifying Checksum\n",
      "0b38311ee46d: Download complete\n",
      "36e73f53893e: Verifying Checksum\n",
      "36e73f53893e: Download complete\n",
      "41c3544d11de: Verifying Checksum\n",
      "41c3544d11de: Download complete\n",
      "29731ba486cc: Verifying Checksum\n",
      "29731ba486cc: Download complete\n",
      "266a99828791: Verifying Checksum\n",
      "266a99828791: Download complete\n",
      "2ca38bff7b4b: Verifying Checksum\n",
      "2ca38bff7b4b: Download complete\n",
      "090a178c469c: Verifying Checksum\n",
      "090a178c469c: Download complete\n",
      "34c760d06da9: Verifying Checksum\n",
      "34c760d06da9: Download complete\n",
      "07e648356d7b: Verifying Checksum\n",
      "07e648356d7b: Download complete\n",
      "5eb92d55fd4b: Verifying Checksum\n",
      "5eb92d55fd4b: Download complete\n",
      "ebfd527d91dd: Verifying Checksum\n",
      "ebfd527d91dd: Download complete\n",
      "2c11b7cecaa5: Pull complete\n",
      "ab02fbc06098: Download complete\n",
      "6bd7c5ee4c13: Verifying Checksum\n",
      "6bd7c5ee4c13: Download complete\n",
      "04637fa56252: Pull complete\n",
      "d6e6af23a0f3: Pull complete\n",
      "b4a424de92ad: Pull complete\n",
      "59fdb7d45b6c: Pull complete\n",
      "d27528dc2d9f: Verifying Checksum\n",
      "d27528dc2d9f: Download complete\n",
      "7759500808dd: Pull complete\n",
      "89fa8d1cd3c8: Pull complete\n",
      "41c3544d11de: Pull complete\n",
      "a033a60349a3: Verifying Checksum\n",
      "a033a60349a3: Download complete\n",
      "a3320505acfb: Verifying Checksum\n",
      "a3320505acfb: Download complete\n",
      "c478ef85c719: Verifying Checksum\n",
      "c478ef85c719: Download complete\n",
      "a47ff4dd1721: Download complete\n",
      "7287905e7101: Download complete\n",
      "b38df7065dd7: Verifying Checksum\n",
      "b38df7065dd7: Download complete\n",
      "e1358f07d66b: Verifying Checksum\n",
      "e1358f07d66b: Download complete\n",
      "b47d8fe99d7a: Verifying Checksum\n",
      "b47d8fe99d7a: Download complete\n",
      "a5faaecb2165: Verifying Checksum\n",
      "a5faaecb2165: Download complete\n",
      "a14804e0781b: Download complete\n",
      "cb5b6ad7922b: Verifying Checksum\n",
      "cb5b6ad7922b: Download complete\n",
      "e171fd9dd068: Verifying Checksum\n",
      "e171fd9dd068: Download complete\n",
      "c154e275cfdf: Download complete\n",
      "cb32ad90adc6: Download complete\n",
      "32634d002d01: Pull complete\n",
      "6bf34d10d7a4: Verifying Checksum\n",
      "6bf34d10d7a4: Download complete\n",
      "10464939f218: Verifying Checksum\n",
      "3d8caa4d24d5: Pull complete\n",
      "0b38311ee46d: Pull complete\n",
      "36e73f53893e: Pull complete\n",
      "d27528dc2d9f: Pull complete\n",
      "2ca38bff7b4b: Pull complete\n",
      "29731ba486cc: Pull complete\n",
      "266a99828791: Pull complete\n",
      "ab02fbc06098: Pull complete\n",
      "090a178c469c: Pull complete\n",
      "34c760d06da9: Pull complete\n",
      "07e648356d7b: Pull complete\n",
      "5eb92d55fd4b: Pull complete\n",
      "ebfd527d91dd: Pull complete\n",
      "cd9320933d55: Verifying Checksum\n",
      "cd9320933d55: Download complete\n",
      "cd9320933d55: Pull complete\n",
      "6bd7c5ee4c13: Pull complete\n",
      "a033a60349a3: Pull complete\n",
      "10464939f218: Pull complete\n",
      "a3320505acfb: Pull complete\n",
      "c478ef85c719: Pull complete\n",
      "a47ff4dd1721: Pull complete\n",
      "7287905e7101: Pull complete\n",
      "b38df7065dd7: Pull complete\n",
      "e1358f07d66b: Pull complete\n",
      "b47d8fe99d7a: Pull complete\n",
      "a5faaecb2165: Pull complete\n",
      "a14804e0781b: Pull complete\n",
      "cb5b6ad7922b: Pull complete\n",
      "EntityType feature values imported. Resource name: projects/106131389347/locations/us-central1/featurestores/mobile_gaming/entityTypes/user\n",
      "e171fd9dd068: Pull complete\n",
      "c154e275cfdf: Pull complete\n",
      "cb32ad90adc6: Pull complete\n",
      "6bf34d10d7a4: Pull complete\n",
      "Digest: sha256:228dfab7fd1dcf674b798e402db5b04bc5346bd0a7865dba3f93dde60fe8cd83\n",
      "Status: Downloaded newer image for us-docker.pkg.dev/vertex-ai/training/scikit-learn-cpu.0-23:latest\n",
      " ---> 6da3b9be283f\n",
      "Step 2/13 : RUN mkdir -m 777 -p /usr/app /home\n",
      " ---> Running in 099e413c0b12\n",
      "Removing intermediate container 099e413c0b12\n",
      " ---> 267d52a78694\n",
      "Step 3/13 : WORKDIR /usr/app\n",
      " ---> Running in 8a34745fdcef\n",
      "Removing intermediate container 8a34745fdcef\n",
      " ---> 2f38db19fdf4\n",
      "Step 4/13 : ENV HOME=/home\n",
      " ---> Running in 7932109b5fad\n",
      "Removing intermediate container 7932109b5fad\n",
      " ---> edc008c90ef4\n",
      "Step 5/13 : ENV PYTHONDONTWRITEBYTECODE=1\n",
      " ---> Running in 5b5f79878590\n",
      "Removing intermediate container 5b5f79878590\n",
      " ---> 947e7dc62c97\n",
      "Step 6/13 : RUN rm -rf /var/sitecustomize\n",
      " ---> Running in 3aa49d05a1af\n",
      "Removing intermediate container 3aa49d05a1af\n",
      " ---> 7f06b6d41802\n",
      "Step 7/13 : COPY [\"./requirements.txt\", \"./requirements.txt\"]\n",
      " ---> f92f53ea467b\n",
      "Step 8/13 : RUN pip3 install --no-cache-dir -r ./requirements.txt\n",
      " ---> Running in 81a0fbd3fce0\n",
      "Collecting pip==22.0.4\n",
      "  Downloading pip-22.0.4-py3-none-any.whl (2.1 MB)\n",
      "Requirement already satisfied: PyYAML==5.3.1 in /usr/local/lib/python3.7/dist-packages (from -r ./requirements.txt (line 2)) (5.3.1)\n",
      "Requirement already satisfied: joblib==0.15.1 in /usr/local/lib/python3.7/dist-packages (from -r ./requirements.txt (line 3)) (0.15.1)\n",
      "Requirement already satisfied: numpy==1.18.5 in /usr/local/lib/python3.7/dist-packages (from -r ./requirements.txt (line 4)) (1.18.5)\n",
      "Requirement already satisfied: pandas==1.0.4 in /usr/local/lib/python3.7/dist-packages (from -r ./requirements.txt (line 5)) (1.0.4)\n",
      "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from -r ./requirements.txt (line 6)) (1.4.1)\n",
      "Requirement already satisfied: scikit-learn==0.23.1 in /usr/local/lib/python3.7/dist-packages (from -r ./requirements.txt (line 7)) (0.23.1)\n",
      "Requirement already satisfied: xgboost==1.1.1 in /usr/local/lib/python3.7/dist-packages (from -r ./requirements.txt (line 8)) (1.1.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas==1.0.4->-r ./requirements.txt (line 5)) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from pandas==1.0.4->-r ./requirements.txt (line 5)) (2.8.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.1->-r ./requirements.txt (line 7)) (2.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.6.1->pandas==1.0.4->-r ./requirements.txt (line 5)) (1.15.0)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 20.1\n",
      "    Uninstalling pip-20.1:\n",
      "      Successfully uninstalled pip-20.1\n",
      "Successfully installed pip-22.0.4\n",
      "Removing intermediate container 81a0fbd3fce0\n",
      " ---> 181f46aae3d6\n",
      "Step 9/13 : COPY [\"config\", \"config\"]\n",
      " ---> c4af883543d2\n",
      "Step 10/13 : COPY [\"data\", \"data\"]\n",
      " ---> 7a25d2e73354\n",
      "Step 11/13 : COPY [\"model\", \"model\"]\n",
      " ---> 8b5d3b6ea396\n",
      "Step 12/13 : COPY [\"trainer\", \"trainer\"]\n",
      " ---> 6d2cf736339b\n",
      "Step 13/13 : ENTRYPOINT [\"python3\", \"-m\", \"trainer.task\"]\n",
      " ---> Running in f4449dd19dcc\n",
      "Removing intermediate container f4449dd19dcc\n",
      " ---> c8c2d5b3263c\n",
      "Successfully built c8c2d5b3263c\n",
      "Successfully tagged cloudai-autogenerated/trainer.task.py:20221107.11.33.58.548291\n",
      "A training image is built.\n",
      "Starting to run ...\n",
      "/usr/lib/google-cloud-sdk/platform/bundledpythonunix/lib/python3.9/subprocess.py:935: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdin = io.open(p2cwrite, 'wb', bufsize)\n",
      "/usr/lib/google-cloud-sdk/platform/bundledpythonunix/lib/python3.9/subprocess.py:941: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/usr/app/trainer/task.py\", line 162, in <module>\n",
      "    main()\n",
      "  File \"/usr/app/trainer/task.py\", line 137, in main\n",
      "    x_train, x_test, y_train, y_test = preprocess_data(data_df, data_model_params)\n",
      "  File \"/usr/app/trainer/task.py\", line 92, in preprocess_data\n",
      "    'random_state'])\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py\", line 2131, in train_test_split\n",
      "    default_test_size=0.25)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py\", line 1814, in _validate_shuffle_split\n",
      "    train_size)\n",
      "ValueError: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.\n",
      "Starting training...\n",
      "Reading data...\n",
      "Preprocessing data...\n",
      "\u001b[1;31mERROR:\u001b[0m (gcloud.ai.custom-jobs.local-run) \n",
      "        Docker failed with error code 1.\n",
      "        Command: docker run --rm -v /home/jupyter/.config/gcloud/application_default_credentials.json:/tmp/keys/cred_key.json -e GOOGLE_APPLICATION_CREDENTIALS=/tmp/keys/cred_key.json --ipc host cloudai-autogenerated/trainer.task.py:20221107.11.33.58.548291 --data_path data/raw/sample.csv --model_dir model --config_path config/config.yaml\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "!chmod +x ./local_train_job_run.sh && ./local_train_job_run.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Z1Kwg6pe0Jx"
   },
   "source": [
    "### Create and Launch the Custom training pipeline to train the model with `autopackaging`.\n",
    "\n",
    "You use `autopackaging` from Vertex AI SDK in order to \n",
    "\n",
    "1. Build a custom Docker training image.\n",
    "2. Push the image to Container Registry.\n",
    "3. Start a Vertex AI CustomJob.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "xcnEOZKywc_8"
   },
   "outputs": [],
   "source": [
    "!mkdir -m 777 -p {MODEL_PACKAGE_PATH} && mv -t {MODEL_PACKAGE_PATH} trainer requirements.txt config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "XQ4koRVCwc_8"
   },
   "outputs": [],
   "source": [
    "train_job_script = f\"\"\"\n",
    "gcloud ai custom-jobs create \\\n",
    "--region={REGION} \\\n",
    "--display-name={TRAIN_JOB_NAME} \\\n",
    "--worker-pool-spec=machine-type={TRAINING_MACHINE_TYPE},replica-count={TRAINING_REPLICA_COUNT},executor-image-uri={BASE_CPU_IMAGE},local-package-path={MODEL_PACKAGE_PATH},python-module=trainer.task,extra-dirs=config \\\n",
    "--args=--data_path={DATA_PATH},--model_dir={MODEL_DIR},--config_path=config/config.yaml \\\n",
    "--verbosity='info'\n",
    "\"\"\"\n",
    "\n",
    "with open(\"train_job_run.sh\", \"w+\") as s:\n",
    "    s.write(train_job_script)\n",
    "s.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "Dwq1Jyklwc_8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "INFO: Running command: docker build --no-cache -t gcr.io/uki-mlops-dev-demo/cloudai-autogenerated/xgb_classifier_training_20220616:20221107.11.39.58.444669 --rm -f- train_package\n",
      "/usr/lib/google-cloud-sdk/platform/bundledpythonunix/lib/python3.9/subprocess.py:935: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdin = io.open(p2cwrite, 'wb', bufsize)\n",
      "/usr/lib/google-cloud-sdk/platform/bundledpythonunix/lib/python3.9/subprocess.py:941: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\n",
      "Sending build context to Docker daemon  12.47kB\n",
      "Step 1/11 : FROM us-docker.pkg.dev/vertex-ai/training/scikit-learn-cpu.0-23:latest\n",
      " ---> 6da3b9be283f\n",
      "Step 2/11 : RUN mkdir -m 777 -p /usr/app /home\n",
      " ---> Running in 6945c6356470\n",
      "Removing intermediate container 6945c6356470\n",
      " ---> 199fbb6af3e1\n",
      "Step 3/11 : WORKDIR /usr/app\n",
      " ---> Running in eaecf6a40b45\n",
      "Removing intermediate container eaecf6a40b45\n",
      " ---> 808d1fdb383f\n",
      "Step 4/11 : ENV HOME=/home\n",
      " ---> Running in b98b61e99506\n",
      "Removing intermediate container b98b61e99506\n",
      " ---> f792744f1f9f\n",
      "Step 5/11 : ENV PYTHONDONTWRITEBYTECODE=1\n",
      " ---> Running in b880028a3908\n",
      "Removing intermediate container b880028a3908\n",
      " ---> c4bc64c9d389\n",
      "Step 6/11 : RUN rm -rf /var/sitecustomize\n",
      " ---> Running in 48b93627a720\n",
      "Removing intermediate container 48b93627a720\n",
      " ---> df723a39a393\n",
      "Step 7/11 : COPY [\"./requirements.txt\", \"./requirements.txt\"]\n",
      " ---> 0c9712d34c8c\n",
      "Step 8/11 : RUN pip3 install --no-cache-dir -r ./requirements.txt\n",
      " ---> Running in 861f388dc831\n",
      "Collecting pip==22.0.4\n",
      "  Downloading pip-22.0.4-py3-none-any.whl (2.1 MB)\n",
      "Requirement already satisfied: PyYAML==5.3.1 in /usr/local/lib/python3.7/dist-packages (from -r ./requirements.txt (line 2)) (5.3.1)\n",
      "Requirement already satisfied: joblib==0.15.1 in /usr/local/lib/python3.7/dist-packages (from -r ./requirements.txt (line 3)) (0.15.1)\n",
      "Requirement already satisfied: numpy==1.18.5 in /usr/local/lib/python3.7/dist-packages (from -r ./requirements.txt (line 4)) (1.18.5)\n",
      "Requirement already satisfied: pandas==1.0.4 in /usr/local/lib/python3.7/dist-packages (from -r ./requirements.txt (line 5)) (1.0.4)\n",
      "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from -r ./requirements.txt (line 6)) (1.4.1)\n",
      "Requirement already satisfied: scikit-learn==0.23.1 in /usr/local/lib/python3.7/dist-packages (from -r ./requirements.txt (line 7)) (0.23.1)\n",
      "Requirement already satisfied: xgboost==1.1.1 in /usr/local/lib/python3.7/dist-packages (from -r ./requirements.txt (line 8)) (1.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from pandas==1.0.4->-r ./requirements.txt (line 5)) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas==1.0.4->-r ./requirements.txt (line 5)) (2020.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.1->-r ./requirements.txt (line 7)) (2.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.6.1->pandas==1.0.4->-r ./requirements.txt (line 5)) (1.15.0)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 20.1\n",
      "    Uninstalling pip-20.1:\n",
      "      Successfully uninstalled pip-20.1\n",
      "Successfully installed pip-22.0.4\n",
      "Removing intermediate container 861f388dc831\n",
      " ---> cb66e92be1f4\n",
      "Step 9/11 : COPY [\"config\", \"config\"]\n",
      " ---> 7ec4d2c31d9c\n",
      "Step 10/11 : COPY [\"trainer\", \"trainer\"]\n",
      " ---> b44e46c07c21\n",
      "Step 11/11 : ENTRYPOINT [\"python3\", \"-m\", \"trainer.task\"]\n",
      " ---> Running in 46e902ed8542\n",
      "Removing intermediate container 46e902ed8542\n",
      " ---> 250d79b90382\n",
      "Successfully built 250d79b90382\n",
      "Successfully tagged gcr.io/uki-mlops-dev-demo/cloudai-autogenerated/xgb_classifier_training_20220616:20221107.11.39.58.444669\n",
      "\n",
      "A custom container image is built locally.\n",
      "\n",
      "INFO: Running command: docker push gcr.io/uki-mlops-dev-demo/cloudai-autogenerated/xgb_classifier_training_20220616:20221107.11.39.58.444669\n",
      "/usr/lib/google-cloud-sdk/platform/bundledpythonunix/lib/python3.9/subprocess.py:935: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdin = io.open(p2cwrite, 'wb', bufsize)\n",
      "/usr/lib/google-cloud-sdk/platform/bundledpythonunix/lib/python3.9/subprocess.py:941: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\n",
      "The push refers to repository [gcr.io/uki-mlops-dev-demo/cloudai-autogenerated/xgb_classifier_training_20220616]\n",
      "56e9038c9b24: Preparing\n",
      "182801ac4a50: Preparing\n",
      "3e7b0ed1287d: Preparing\n",
      "6eba6c4280ce: Preparing\n",
      "f4215792d039: Preparing\n",
      "6a871babef09: Preparing\n",
      "4c57c3a2fee3: Preparing\n",
      "4c57c3a2fee3: Preparing\n",
      "296c56a6ee6b: Preparing\n",
      "2c438d738d60: Preparing\n",
      "12a5d56ad978: Preparing\n",
      "f2650c7a7c64: Preparing\n",
      "b1a9ba8760fd: Preparing\n",
      "f2650c7a7c64: Preparing\n",
      "ec03677c729a: Preparing\n",
      "78c3171c43e6: Preparing\n",
      "6ae1ebeae50a: Preparing\n",
      "c34a4d8294c8: Preparing\n",
      "8276fea1eb8b: Preparing\n",
      "a58c076686c8: Preparing\n",
      "48116b9cec2b: Preparing\n",
      "53fb146beab8: Preparing\n",
      "5f5d731f19ba: Preparing\n",
      "d559ad01f5c9: Preparing\n",
      "8944cb7f88be: Preparing\n",
      "7323907347ef: Preparing\n",
      "f1ebba1c1e58: Preparing\n",
      "e6708a930863: Preparing\n",
      "067985996699: Preparing\n",
      "d78b608be574: Preparing\n",
      "9ebddcddc12d: Preparing\n",
      "da8cdd8bbfd7: Preparing\n",
      "6a871babef09: Waiting\n",
      "71725e24bfeb: Preparing\n",
      "f89fb4ac64eb: Preparing\n",
      "4c57c3a2fee3: Waiting\n",
      "ed36ac5b8b67: Preparing\n",
      "296c56a6ee6b: Waiting\n",
      "8c2fd7b74873: Preparing\n",
      "1a36d078ca8f: Preparing\n",
      "2c438d738d60: Waiting\n",
      "e638ab181060: Preparing\n",
      "ebd667352663: Preparing\n",
      "12a5d56ad978: Waiting\n",
      "f2773ea7b65a: Preparing\n",
      "f2650c7a7c64: Waiting\n",
      "bbb8bc7f0e93: Preparing\n",
      "7a7fcf89ef4a: Preparing\n",
      "b1a9ba8760fd: Waiting\n",
      "a87311dc7421: Preparing\n",
      "ec03677c729a: Waiting\n",
      "5ae9bc7f38fb: Preparing\n",
      "9edaa71ce233: Preparing\n",
      "78c3171c43e6: Waiting\n",
      "62fdddf6a67c: Preparing\n",
      "eff16de3ff64: Preparing\n",
      "6ae1ebeae50a: Waiting\n",
      "61727f5e6796: Preparing\n",
      "c34a4d8294c8: Waiting\n",
      "5f5d731f19ba: Waiting\n",
      "8276fea1eb8b: Waiting\n",
      "d559ad01f5c9: Waiting\n",
      "8944cb7f88be: Waiting\n",
      "a58c076686c8: Waiting\n",
      "48116b9cec2b: Waiting\n",
      "53fb146beab8: Waiting\n",
      "e638ab181060: Waiting\n",
      "7323907347ef: Waiting\n",
      "f1ebba1c1e58: Waiting\n",
      "f2773ea7b65a: Waiting\n",
      "9edaa71ce233: Waiting\n",
      "bbb8bc7f0e93: Waiting\n",
      "7a7fcf89ef4a: Waiting\n",
      "e6708a930863: Waiting\n",
      "a87311dc7421: Waiting\n",
      "5ae9bc7f38fb: Waiting\n",
      "62fdddf6a67c: Waiting\n",
      "067985996699: Waiting\n",
      "eff16de3ff64: Waiting\n",
      "f89fb4ac64eb: Waiting\n",
      "61727f5e6796: Waiting\n",
      "ed36ac5b8b67: Waiting\n",
      "1a36d078ca8f: Waiting\n",
      "d78b608be574: Waiting\n",
      "da8cdd8bbfd7: Waiting\n",
      "71725e24bfeb: Waiting\n",
      "8c2fd7b74873: Waiting\n",
      "182801ac4a50: Pushed\n",
      "f4215792d039: Pushed\n",
      "56e9038c9b24: Pushed\n",
      "6eba6c4280ce: Pushed\n",
      "3e7b0ed1287d: Pushed\n",
      "6a871babef09: Pushed\n",
      "4c57c3a2fee3: Pushed\n",
      "2c438d738d60: Pushed\n",
      "296c56a6ee6b: Pushed\n",
      "f2650c7a7c64: Pushed\n",
      "ec03677c729a: Pushed\n",
      "b1a9ba8760fd: Pushed\n",
      "78c3171c43e6: Pushed\n",
      "12a5d56ad978: Pushed\n",
      "6ae1ebeae50a: Pushed\n",
      "c34a4d8294c8: Pushed\n",
      "8276fea1eb8b: Pushed\n",
      "a58c076686c8: Pushed\n",
      "48116b9cec2b: Pushed\n",
      "53fb146beab8: Pushed\n",
      "8944cb7f88be: Pushed\n",
      "7323907347ef: Pushed\n",
      "f1ebba1c1e58: Pushed\n",
      "e6708a930863: Pushed\n",
      "9ebddcddc12d: Pushed\n",
      "d78b608be574: Pushed\n",
      "71725e24bfeb: Pushed\n",
      "f89fb4ac64eb: Pushed\n",
      "ed36ac5b8b67: Pushed\n",
      "067985996699: Pushed\n",
      "1a36d078ca8f: Pushed\n",
      "e638ab181060: Pushed\n",
      "ebd667352663: Pushed\n",
      "da8cdd8bbfd7: Pushed\n",
      "f2773ea7b65a: Pushed\n",
      "7a7fcf89ef4a: Pushed\n",
      "a87311dc7421: Pushed\n",
      "bbb8bc7f0e93: Pushed\n",
      "9edaa71ce233: Layer already exists\n",
      "62fdddf6a67c: Layer already exists\n",
      "eff16de3ff64: Layer already exists\n",
      "61727f5e6796: Layer already exists\n",
      "5ae9bc7f38fb: Pushed\n",
      "8c2fd7b74873: Pushed\n",
      "5f5d731f19ba: Pushed\n",
      "d559ad01f5c9: Pushed\n",
      "20221107.11.39.58.444669: digest: sha256:97c6e183d17ad06c753cb683b542ce834bdfaee4bc4fcf64382a748a74d9633a size: 10338\n",
      "\n",
      "Custom container image [gcr.io/uki-mlops-dev-demo/cloudai-autogenerated/xgb_classifier_training_20220616:20221107.11.39.58.444669] is created for your custom job.\n",
      "\n",
      "CustomJob [projects/106131389347/locations/us-central1/customJobs/1667587229528096768] is submitted successfully.\n",
      "\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai custom-jobs describe projects/106131389347/locations/us-central1/customJobs/1667587229528096768\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai custom-jobs stream-logs projects/106131389347/locations/us-central1/customJobs/1667587229528096768\n",
      "INFO: Display format: \"none\"\n"
     ]
    }
   ],
   "source": [
    "!chmod +x ./train_job_run.sh && ./train_job_run.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3olHyo5MpPaC"
   },
   "source": [
    "### Check the status of training job and the result. \n",
    "\n",
    "You can use the following commands to monitor the status of your job and check for the artefact in the bucket once the training successfully run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "Yh6-mWPhpfD_"
   },
   "outputs": [],
   "source": [
    "TRAIN_JOB_RESOURCE_NAME = \"projects/106131389347/locations/us-central1/customJobs/1667587229528096768\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "jpOA6aw5wc_8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "createTime: '2022-11-07T11:41:49.938821Z'\n",
      "displayName: xgb_classifier_training_20220616\n",
      "endTime: '2022-11-07T12:19:14Z'\n",
      "error:\n",
      "  code: 3\n",
      "  message: 'The replica workerpool0-0 exited with a non-zero status of 1. To find\n",
      "    out more about why your job exited please check the logs: https://console.cloud.google.com/logs/viewer?project=106131389347&resource=ml_job%2Fjob_id%2F1667587229528096768&advancedFilter=resource.type%3D%22ml_job%22%0Aresource.labels.job_id%3D%221667587229528096768%22'\n",
      "jobSpec:\n",
      "  workerPoolSpecs:\n",
      "  - containerSpec:\n",
      "      args:\n",
      "      - --data_path=/gcs/vertex-experiments-dl/data/features/train_features_20220616/000000000000.csv\n",
      "      - --model_dir=/gcs/vertex-experiments-dl/model/20220616\n",
      "      - --config_path=config/config.yaml\n",
      "      imageUri: gcr.io/uki-mlops-dev-demo/cloudai-autogenerated/xgb_classifier_training_20220616:20221107.11.39.58.444669\n",
      "    diskSpec:\n",
      "      bootDiskSizeGb: 100\n",
      "      bootDiskType: pd-ssd\n",
      "    machineSpec:\n",
      "      machineType: n1-standard-4\n",
      "    replicaCount: '1'\n",
      "name: projects/106131389347/locations/us-central1/customJobs/1667587229528096768\n",
      "startTime: '2022-11-07T12:19:13Z'\n",
      "state: JOB_STATE_FAILED\n",
      "updateTime: '2022-11-07T12:20:53.900183Z'\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai custom-jobs describe $TRAIN_JOB_RESOURCE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lyborm0Myut5"
   },
   "outputs": [],
   "source": [
    "!gsutil ls $DESTINATION_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tC9fkIMBwc_8"
   },
   "source": [
    "### Upload and Deploy Model on Vertex AI Endpoint\n",
    "\n",
    "You use a custom function to upload your model to a Vertex AI Model Registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W17gaIjtwc_8"
   },
   "outputs": [],
   "source": [
    "xgb_model = upload_model(\n",
    "    display_name=MODEL_NAME,\n",
    "    serving_container_image_uri=SERVING_CONTAINER_IMAGE_URI,\n",
    "    artifact_uri=DESTINATION_URI,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWZmWjnye4N5"
   },
   "source": [
    "### Deploy Model to the same Endpoint with Traffic Splitting\n",
    "\n",
    "Now that you have registered in the model registry, you can deploy it in an endpoint. So you firstly create the endpoint and then you deploy your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZWpDZVtmwc_9"
   },
   "outputs": [],
   "source": [
    "endpoint = create_endpoint(display_name=ENDPOINT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Nw4o2fJwc_9"
   },
   "outputs": [],
   "source": [
    "deployed_model = deploy_model(\n",
    "    model=xgb_model,\n",
    "    machine_type=SERVING_MACHINE_TYPE,\n",
    "    endpoint=endpoint,\n",
    "    deployed_model_display_name=DEPLOYED_MODEL_NAME,\n",
    "    min_replica_count=1,\n",
    "    max_replica_count=1,\n",
    "    sync=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1TNzL_EGrVUm"
   },
   "source": [
    "# Serve ML features at scale with low latency\n",
    " \n",
    "At that time, you are ready **to deploy our simple model which would requires fetching preprocessed attributes as input features in real time**.\n",
    " \n",
    "Below you can see how it works\n",
    " \n",
    "<center><img src=\"./online_serving_5.png\" width=\"800\"/></center>\n",
    " \n",
    "But think about those features for a second.\n",
    " \n",
    "Your behavioral features used to train your model, they cannot be computed when you are going to serve the model online.\n",
    " \n",
    "How could you compute the number of times a user challenged a friend within the last 24 hours on the fly?\n",
    " \n",
    "You need to be computed this feature on the server side and serve it with low latency. And because Bigquery is not optimized for those read operations, we need a different service that allows singleton lookup where the result is a single row with many columns.\n",
    " \n",
    "Also, even if it was not the case, when you deploy a model that requires preprocessing your data, you need to be sure to reproduce the same preprocessing steps you had when you trained it. If you are not able to do that a skew between training and serving data would happen and it will badly affect your model performance (and in the worst scenario break your serving system).\n",
    " \n",
    "You need a way to mitigate that in a way you don't need to implement those preprocessing steps online but just serve the same aggregated features you already have for training to generate online prediction.\n",
    " \n",
    "These are other valuable reasons to introduce Vertex AI Feature Store. With it, you have a service which helps you to serve features at scale with low latency as they were available at training time mitigating in that way possible training-serving skew.\n",
    " \n",
    "Now that you know **why you need a feature store**, let's conclude this journey by deploying your model using a feature store to retrieve features online, pass them to the endpoint and generate predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1e600b031a50"
   },
   "source": [
    "## Time to simulate online predictions\n",
    "\n",
    "Once the model is ready to receive prediction requests, you can use the `simulate_prediction` function to generate them. \n",
    "\n",
    "In particular, that function\n",
    "\n",
    "- format entities for prediction\n",
    "- retrieve static features with a singleton lookup operations from Vertex AI Feature store\n",
    "- run the prediction request and get back the result\n",
    "\n",
    "for a number of requests and some latency you define. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k42QhcjcG5pe"
   },
   "outputs": [],
   "source": [
    "simulate_prediction(endpoint=endpoint, n_requests=10, latency=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8d3S1d1urZOy"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NESfOgK5nkNu"
   },
   "outputs": [],
   "source": [
    "# delete feature store\n",
    "mobile_gaming_feature_store.delete(sync=True, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4W28-97kfPDb"
   },
   "outputs": [],
   "source": [
    "# delete Vertex AI resources\n",
    "endpoint.undeploy_all()\n",
    "xgb_model.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FMXT2akXrZOy"
   },
   "outputs": [],
   "source": [
    "# Delete bucket\n",
    "delete_bucket = False\n",
    "if (delete_bucket or os.getenv(\"IS_TESTING\")) and \"BUCKET_URI\" in globals():\n",
    "    ! gsutil -m rm -r $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oLVhx7bQeJ6H"
   },
   "outputs": [],
   "source": [
    "# Delete the BigQuery Dataset\n",
    "!bq rm -r -f -d $PROJECT_ID:$BQ_DATASET"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "mobile_gaming_feature_store.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m99",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m99"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
