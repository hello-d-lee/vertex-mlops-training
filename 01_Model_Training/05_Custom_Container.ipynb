{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86415322-9ac3-4c4b-b8c9-055d7220eaa8",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this section, we'll see how to create and use a custom container for a Vertex AI training job \n",
    "\n",
    "Create a Custom Container for Vertex AI pipeline model training\n",
    "1. Create a Python model trainer module using the model_sample.py file provided - YOU WILL NEED TO CHANGE THE VARIABLES FOR THIS TO RUN PROPERLY - e.g. the project and bucket information\n",
    "2. Save your code as `model.py` in the `model/trainer` beneath the current working directory for this notebook\n",
    "3. Make sure you set the Project ID correctly in the Python script. \n",
    "4. Create the Dockerfile definition in the `model/` directory for your custom training container using the `gcr.io/deeplearning-platform-release/tf2-cpu.2-6` base container image\n",
    "\n",
    "Once you have prepared the custom container Python module code and Dockerfile you can build and test the custom container. \n",
    "\n",
    "Optionally, you can test how to use this custom container in training pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7099631f-c290-4951-8f6d-782f16d156e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install google-cloud-aiplatform --user --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f5af45-d44f-439c-859a-0a44be6c192f",
   "metadata": {},
   "source": [
    "In order to create the container and run this training job, you first need to get the training data moved into your own Google Cloud Storage bucket. Then, you'll need to update the corresponding variables in the model.py script to point to the proper location / region\n",
    "\n",
    "The training source data can be downloaded from this repository as:\n",
    "area_cover_dataset.csv\n",
    "\n",
    "You may need to create a GCP Container Registry if you do not have an existing one to use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55345ca8-83c0-4e6e-860b-63c099976f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the modules used by the training job in the model.py script\n",
    "import tensorflow \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import keras_tuner \n",
    "from google.cloud import aiplatform\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import numpy\n",
    "import pandas\n",
    "import json, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9e140c-514b-4b01-ad70-cd5c4e21861d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the variables you will need to change - both here and in your model.py script\n",
    "REGION = \"us-central1\"\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "MODEL_PATH='gs://'+PROJECT_ID+'-bucket/model'\n",
    "DATASET_PATH='gs://'+PROJECT_ID+'-bucket/area_cover_dataset.csv'\n",
    "PIPELINE_ROOT = 'gs://'+PROJECT_ID+'-bucket'\n",
    "MODEL_ARTIFACTS_LOCATION ='gs://'+PROJECT_ID+'-bucket/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49c0172-944d-4cf3-9da1-4925d6c6e647",
   "metadata": {},
   "source": [
    "Once you have updated all of your variables, you're ready to start building the container, testing it, and then running the training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bda3517-75cf-4244-83fc-6d7d83623d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the container using the following gcr.io tag\n",
    "IMAGE_URI=\"gcr.io/{}/tensorflow:latest\".format(PROJECT_ID)\n",
    "!docker build ~/model/. -t $IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fda9644-7b7c-4738-914d-ae097aa99140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the docker image locally to test it\n",
    "!docker run $IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66602a9-8c16-4a20-a6b5-98a2b0e4af38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push the docker image to the Google container registry\n",
    "!docker push $IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e88053-413f-439d-b586-799ddde309d6",
   "metadata": {},
   "source": [
    "You can navigate to the Container Registry to see the image created successfully - this is also where you can get its URI\n",
    "\n",
    "Now, this can be used as part of a training job or pipeline. See example below - may make more sense to come back to this when you've completed the basic pipelines 101 section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82a6527-b30a-40f0-867c-d0ee06bf16bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install kubeflow pipeline SDK and google cloud pipeline component for building Vertex AI pipelines\n",
    "!pip3 install kfp google_cloud_pipeline_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7deb3a-90df-4a8d-8571-63246eaff007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries required for Vertext AI pipelines\n",
    "import kfp\n",
    "from kfp.v2 import compiler\n",
    "from google.cloud import aiplatform\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac848c03-0c7e-4f03-8bf4-00c9e97ea813",
   "metadata": {},
   "source": [
    "If you want to test running the pipeline yourself:\n",
    "* Make sure to update the container_uri to use the custom container URI that you created in the previous steps\n",
    "* You will also want to update the base_output_dir location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4703742c-1427-4d3e-af1c-f27998321167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Vertex AI pipeline\n",
    "@kfp.dsl.pipeline(name=\"vertex-ai-pipeline\",\n",
    "                  pipeline_root=PIPELINE_ROOT)\n",
    "def pipeline(\n",
    "    bucket: str = MODEL_PATH,\n",
    "    project: str = PROJECT_ID,\n",
    "    gcp_region: str = REGION,\n",
    "    container_uri: str = \"gcr.io/uki-mlops-dev-demo/tensorflow@sha256:e3f9f2c4bc1879b864f2931416d7c6d6a78a36d7493222a98ff39afc679a8f81\",\n",
    "):\n",
    "    \n",
    "    training_op = gcc_aip.CustomContainerTrainingJobRunOp(\n",
    "        display_name=\"forestcover-train\",\n",
    "        container_uri=container_uri,\n",
    "        project=project,\n",
    "        location=gcp_region,\n",
    "        staging_bucket=bucket,\n",
    "        base_output_dir=\"gs://uki-mlops-dev-demo-bucket\",\n",
    "        training_fraction_split=0.8,\n",
    "        validation_fraction_split=0.1,\n",
    "        test_fraction_split=0.1,\n",
    "        model_serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-6:latest\",\n",
    "        model_display_name=\"forestcover\",\n",
    "        machine_type=\"n1-standard-4\",\n",
    "    )   \n",
    "    \n",
    "    create_endpoint_op = gcc_aip.EndpointCreateOp(\n",
    "        project=project,\n",
    "        display_name = \"forestcover-endpoint\",\n",
    "    )\n",
    "    \n",
    "    model_deploy_op = gcc_aip.ModelDeployOp(\n",
    "        endpoint=create_endpoint_op.outputs[\"endpoint\"],\n",
    "        model=training_op.outputs[\"model\"],\n",
    "        deployed_model_display_name=\"forestcover\",\n",
    "        dedicated_resources_machine_type=\"n1-standard-4\",\n",
    "        dedicated_resources_min_replica_count=1,\n",
    "        dedicated_resources_max_replica_count=1,   \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f79fe2-5643-4557-bb02-7a6bbb11fdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the  Vertex AI pipeline\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=\"pipeline.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0ea35f-1bd8-4660-a3af-c13ed016d3b1",
   "metadata": {},
   "source": [
    "You can use a timestamp for debugging pipeline runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac39c788-288e-4380-917a-bfcb79391283",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029b06a6-2cc2-43a2-8137-ac8f5ba3e6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Vertex AI Pipeline job object\n",
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name=\"forest-cover\",\n",
    "    template_path=\"pipeline.json\",\n",
    "    job_id=\"forest-train-pipeline-{0}\".format(TIMESTAMP),\n",
    "    parameter_values={\n",
    "        \"project\": PROJECT_ID,\n",
    "        \"bucket\": MODEL_PATH,\n",
    "        \"gcp_region\": REGION,\n",
    "        \"container_uri\": \"gcr.io/uki-mlops-dev-demo/tensorflow@sha256:e3f9f2c4bc1879b864f2931416d7c6d6a78a36d7493222a98ff39afc679a8f81\"\n",
    "    },\n",
    "    enable_caching=True,  \n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e900a7e9-17cb-4cc0-8177-39051a79181c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Vertex AI pipeline job\n",
    "pipeline_job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf83120-047d-4f77-b2ac-5043c1163c9e",
   "metadata": {},
   "source": [
    "Now if you navigate to the Pipelines UI - you'll see the pipeline job running, when it's finished you'll also see the endpoint and model successfully deployed to the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda6cd88-3259-425a-8d53-b611c6942b84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m99",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m99"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
